"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8066],{8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}},9812:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"reference/cli/benchmark","title":"trainloop benchmark","description":"Compare multiple LLM providers by re-running prompts and evaluating results with the same metrics.","source":"@site/docs/reference/cli/benchmark.md","sourceDirName":"reference/cli","slug":"/reference/cli/benchmark","permalink":"/reference/cli/benchmark","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/reference/cli/benchmark.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"docsSidebar","previous":{"title":"trainloop add","permalink":"/reference/cli/add"},"next":{"title":"Configuration","permalink":"/reference/cli/config"}}');var r=s(4848),t=s(8453);const l={sidebar_position:6},a="trainloop benchmark",o={},c=[{value:"Synopsis",id:"synopsis",level:2},{value:"Description",id:"description",level:2},{value:"Options",id:"options",level:2},{value:"Examples",id:"examples",level:2},{value:"Basic Benchmarking",id:"basic-benchmarking",level:3},{value:"Limited Sampling",id:"limited-sampling",level:3},{value:"Specific Tags",id:"specific-tags",level:3},{value:"Custom Providers",id:"custom-providers",level:3},{value:"Configuration",id:"configuration",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Output",id:"output",level:2},{value:"Console Output",id:"console-output",level:3},{value:"Results Structure",id:"results-structure",level:3},{value:"Results Content",id:"results-content",level:3},{value:"Analysis in Studio UI",id:"analysis-in-studio-ui",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Representative Sampling",id:"1-representative-sampling",level:3},{value:"2. Consistent Configuration",id:"2-consistent-configuration",level:3},{value:"3. Cost Management",id:"3-cost-management",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"API Rate Limits",id:"api-rate-limits",level:3},{value:"Insufficient Data",id:"insufficient-data",level:3},{value:"Provider Errors",id:"provider-errors",level:3},{value:"See Also",id:"see-also",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"trainloop-benchmark",children:"trainloop benchmark"})}),"\n",(0,r.jsx)(n.p,{children:"Compare multiple LLM providers by re-running prompts and evaluating results with the same metrics."}),"\n",(0,r.jsx)(n.h2,{id:"synopsis",children:"Synopsis"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"trainloop benchmark [OPTIONS]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"trainloop benchmark"})," command takes your existing event data and re-runs the same prompts against multiple LLM providers configured in your settings. It then applies your evaluation metrics to all responses, enabling direct comparison of model performance."]}),"\n",(0,r.jsx)(n.h2,{id:"options",children:"Options"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Option"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--config <path>"})}),(0,r.jsx)(n.td,{children:"Path to configuration file"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--max-samples <number>"})}),(0,r.jsx)(n.td,{children:"Limit number of samples per provider"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--tag <name>"})}),(0,r.jsx)(n.td,{children:"Only benchmark events with specific tag"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--providers <list>"})}),(0,r.jsx)(n.td,{children:"Comma-separated list of providers to test"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--output <path>"})}),(0,r.jsx)(n.td,{children:"Output directory for results"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--verbose"})}),(0,r.jsx)(n.td,{children:"Enable verbose output"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--help"})}),(0,r.jsx)(n.td,{children:"Show help message"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(n.h3,{id:"basic-benchmarking",children:"Basic Benchmarking"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Run benchmark with configured providers\ntrainloop benchmark\n"})}),"\n",(0,r.jsx)(n.h3,{id:"limited-sampling",children:"Limited Sampling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Test with only 100 samples per provider\ntrainloop benchmark --max-samples 100\n"})}),"\n",(0,r.jsx)(n.h3,{id:"specific-tags",children:"Specific Tags"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Only benchmark greeting generation\ntrainloop benchmark --tag greeting-generation\n"})}),"\n",(0,r.jsx)(n.h3,{id:"custom-providers",children:"Custom Providers"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Test specific providers\ntrainloop benchmark --providers openai/gpt-4o,anthropic/claude-3-sonnet\n"})}),"\n",(0,r.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["Configure benchmark providers in ",(0,r.jsx)(n.code,{children:"trainloop.config.yaml"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"trainloop:\n  benchmark:\n    providers:\n      - provider: openai\n        model: gpt-4o\n        temperature: 0.7\n        max_tokens: 1000\n      - provider: openai\n        model: gpt-4o-mini\n        temperature: 0.7\n        max_tokens: 1000\n      - provider: anthropic\n        model: claude-3-5-sonnet-20241022\n        temperature: 0.7\n        max_tokens: 1000\n    \n    # Optional settings\n    max_samples: 1000\n    parallel_requests: 5\n    timeout: 30\n"})}),"\n",(0,r.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Selection"}),": Selects events from your data based on tags/filters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Provider Execution"}),": Re-runs prompts against each configured provider"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Metric Application"}),": Applies your existing metrics to all responses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Result Generation"}),": Creates comparison data with performance statistics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visualization"}),": Results viewable in Studio UI"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"output",children:"Output"}),"\n",(0,r.jsx)(n.h3,{id:"console-output",children:"Console Output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\ud83d\udd0d Starting benchmark with 3 providers...\n\ud83d\udcca Selected 150 events for benchmarking\n\n\ud83d\ude80 Running prompts against providers:\n\u2705 openai/gpt-4o: 150/150 completed (avg: 1.2s)\n\u2705 openai/gpt-4o-mini: 150/150 completed (avg: 0.8s)\n\u2705 anthropic/claude-3-5-sonnet: 150/150 completed (avg: 1.5s)\n\n\ud83d\udcc8 Applying metrics to results...\n\u2705 helpful_check: Applied to 450 responses\n\u2705 accuracy_check: Applied to 450 responses\n\u2705 safety_check: Applied to 450 responses\n\n\ud83d\udcbe Benchmark results saved to data/benchmarks/2024-01-15_14-30-25/\n"})}),"\n",(0,r.jsx)(n.h3,{id:"results-structure",children:"Results Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"data/\n\u251c\u2500\u2500 benchmarks/\n\u2502   \u2514\u2500\u2500 2024-01-15_14-30-25/\n\u2502       \u251c\u2500\u2500 benchmark_results.json     # Main results\n\u2502       \u251c\u2500\u2500 provider_comparison.json   # Provider stats\n\u2502       \u2514\u2500\u2500 detailed_results.jsonl     # Individual responses\n"})}),"\n",(0,r.jsx)(n.h3,{id:"results-content",children:"Results Content"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "timestamp": "2024-01-15T14:30:25Z",\n  "providers": ["openai/gpt-4o", "openai/gpt-4o-mini", "anthropic/claude-3-5-sonnet"],\n  "total_samples": 150,\n  "metrics": {\n    "helpful_check": {\n      "openai/gpt-4o": {"score": 0.85, "passed": 128, "total": 150},\n      "openai/gpt-4o-mini": {"score": 0.82, "passed": 123, "total": 150},\n      "anthropic/claude-3-5-sonnet": {"score": 0.88, "passed": 132, "total": 150}\n    }\n  },\n  "cost_analysis": {\n    "openai/gpt-4o": {"total_cost": 4.50, "cost_per_token": 0.015},\n    "openai/gpt-4o-mini": {"total_cost": 0.30, "cost_per_token": 0.001}\n  }\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"analysis-in-studio-ui",children:"Analysis in Studio UI"}),"\n",(0,r.jsx)(n.p,{children:"After benchmarking, use Studio UI to analyze results:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"trainloop studio\n"})}),"\n",(0,r.jsx)(n.p,{children:"Features available:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Performance comparison charts"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Cost vs. quality analysis"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Individual response comparison"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Metric breakdown by provider"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Statistical significance testing"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"1-representative-sampling",children:"1. Representative Sampling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Use sufficient samples for statistical significance\ntrainloop benchmark --max-samples 500\n\n# Include diverse event types\ntrainloop benchmark --tag "" # All events\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-consistent-configuration",children:"2. Consistent Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# Use same temperature/settings across providers\nbenchmark:\n  providers:\n    - provider: openai\n      model: gpt-4o\n      temperature: 0.7  # Same across all\n    - provider: anthropic\n      model: claude-3-sonnet\n      temperature: 0.7  # Same across all\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-cost-management",children:"3. Cost Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Start with small samples\ntrainloop benchmark --max-samples 50\n\n# Monitor costs in configuration\nbenchmark:\n  cost_limit: 10.00  # Stop at $10\n"})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"api-rate-limits",children:"API Rate Limits"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Reduce parallel requests\ntrainloop benchmark --parallel 2\n\n# Add delays between requests\ntrainloop benchmark --delay 1.0\n"})}),"\n",(0,r.jsx)(n.h3,{id:"insufficient-data",children:"Insufficient Data"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check available events\nls data/events/\n\n# Verify tags exist\ntrainloop eval --dry-run\n"})}),"\n",(0,r.jsx)(n.h3,{id:"provider-errors",children:"Provider Errors"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Test individual provider\ntrainloop benchmark --providers openai/gpt-4o\n\n# Check API keys\nenv | grep API_KEY\n"})}),"\n",(0,r.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/tutorials/benchmarking",children:"Benchmarking Tutorial"})," - Complete benchmarking guide"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/reference/cli/config",children:"Configuration"})," - Configure benchmark settings"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/reference/cli/studio",children:"Studio UI"})," - Analyze benchmark results"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);