"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5564],{7536:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"tutorials/benchmarking","title":"Benchmarking and Model Comparison","description":"In this tutorial, you\'ll learn how to systematically compare different LLM providers to find the best model for your specific use case. We\'ll cover cost analysis, performance evaluation, and decision-making frameworks.","source":"@site/docs/tutorials/benchmarking.md","sourceDirName":"tutorials","slug":"/tutorials/benchmarking","permalink":"/tutorials/benchmarking","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/tutorials/benchmarking.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docsSidebar","previous":{"title":"Advanced Metrics with LLM Judge","permalink":"/tutorials/advanced-metrics"},"next":{"title":"Production Setup","permalink":"/tutorials/production-setup"}}');var s=i(4848),a=i(8453);const o={sidebar_position:5},t="Benchmarking and Model Comparison",c={},l=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction to Benchmarking",id:"introduction-to-benchmarking",level:2},{value:"Why Benchmark?",id:"why-benchmark",level:3},{value:"What TrainLoop Benchmarking Does",id:"what-trainloop-benchmarking-does",level:3},{value:"Setting Up Benchmarking",id:"setting-up-benchmarking",level:2},{value:"Step 1: Configure API Keys",id:"step-1-configure-api-keys",level:3},{value:"Step 2: Configure Benchmark Providers",id:"step-2-configure-benchmark-providers",level:3},{value:"Step 3: Run Benchmarking",id:"step-3-run-benchmarking",level:3},{value:"Understanding Benchmark Results",id:"understanding-benchmark-results",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Metric-Level Analysis",id:"metric-level-analysis",level:3},{value:"Analyzing Results in Studio UI",id:"analyzing-results-in-studio-ui",level:2},{value:"Launch Studio for Benchmark Analysis",id:"launch-studio-for-benchmark-analysis",level:3},{value:"Key Views for Benchmark Analysis",id:"key-views-for-benchmark-analysis",level:3},{value:"1. Model Comparison Dashboard",id:"1-model-comparison-dashboard",level:4},{value:"2. Sample-Level Analysis",id:"2-sample-level-analysis",level:4},{value:"3. Cost Analysis",id:"3-cost-analysis",level:4},{value:"Advanced Benchmarking Strategies",id:"advanced-benchmarking-strategies",level:2},{value:"1. Stratified Benchmarking",id:"1-stratified-benchmarking",level:3},{value:"2. Domain-Specific Benchmarking",id:"2-domain-specific-benchmarking",level:3},{value:"3. Time-Series Benchmarking",id:"3-time-series-benchmarking",level:3},{value:"Model Selection Framework",id:"model-selection-framework",level:2},{value:"1. Define Your Priorities",id:"1-define-your-priorities",level:3},{value:"2. Normalize Scores",id:"2-normalize-scores",level:3},{value:"3. Decision Matrix",id:"3-decision-matrix",level:3},{value:"Production Benchmarking",id:"production-benchmarking",level:2},{value:"1. Automated Benchmarking",id:"1-automated-benchmarking",level:3},{value:"2. Continuous Monitoring",id:"2-continuous-monitoring",level:3},{value:"3. A/B Testing Framework",id:"3-ab-testing-framework",level:3},{value:"Common Benchmarking Scenarios",id:"common-benchmarking-scenarios",level:2},{value:"1. Cost Optimization",id:"1-cost-optimization",level:3},{value:"2. Accuracy Optimization",id:"2-accuracy-optimization",level:3},{value:"3. Speed Optimization",id:"3-speed-optimization",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Start Small, Scale Up",id:"1-start-small-scale-up",level:3},{value:"2. Use Representative Data",id:"2-use-representative-data",level:3},{value:"3. Regular Benchmarking",id:"3-regular-benchmarking",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"1. API Rate Limits",id:"1-api-rate-limits",level:4},{value:"2. Inconsistent Results",id:"2-inconsistent-results",level:4},{value:"3. Cost Concerns",id:"3-cost-concerns",level:4},{value:"Next Steps",id:"next-steps",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"benchmarking-and-model-comparison",children:"Benchmarking and Model Comparison"})}),"\n",(0,s.jsx)(n.p,{children:"In this tutorial, you'll learn how to systematically compare different LLM providers to find the best model for your specific use case. We'll cover cost analysis, performance evaluation, and decision-making frameworks."}),"\n",(0,s.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How to set up benchmarking for multiple LLM providers"}),"\n",(0,s.jsx)(n.li,{children:"Cost vs. performance analysis techniques"}),"\n",(0,s.jsx)(n.li,{children:"How to interpret benchmark results"}),"\n",(0,s.jsx)(n.li,{children:"Best practices for model selection"}),"\n",(0,s.jsx)(n.li,{children:"How to track performance over time"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Completed ",(0,s.jsx)(n.a,{href:"/tutorials/advanced-metrics",children:"Advanced Metrics with LLM Judge"})]}),"\n",(0,s.jsx)(n.li,{children:"API keys for multiple LLM providers"}),"\n",(0,s.jsx)(n.li,{children:"Existing evaluation metrics and test suites"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-benchmarking",children:"Introduction to Benchmarking"}),"\n",(0,s.jsx)(n.h3,{id:"why-benchmark",children:"Why Benchmark?"}),"\n",(0,s.jsx)(n.p,{children:"Benchmarking helps you:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choose the right model"})," - Find the best performer for your use case"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize costs"})," - Balance performance with API costs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Track improvements"})," - Monitor how new models perform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Make data-driven decisions"})," - Replace guesswork with evidence"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"what-trainloop-benchmarking-does",children:"What TrainLoop Benchmarking Does"}),"\n",(0,s.jsx)(n.p,{children:"TrainLoop's benchmarking feature:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Re-runs your prompts"})," against multiple LLM providers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applies your existing metrics"})," to all provider responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generates comparison reports"})," with performance and cost data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visualizes results"})," in Studio UI for easy analysis"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-benchmarking",children:"Setting Up Benchmarking"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-configure-api-keys",children:"Step 1: Configure API Keys"}),"\n",(0,s.jsx)(n.p,{children:"First, ensure you have API keys for the providers you want to test:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create or update your .env file\ncat > .env << 'EOF'\nOPENAI_API_KEY=your-openai-key-here\nANTHROPIC_API_KEY=your-anthropic-key-here\nGOOGLE_API_KEY=your-google-key-here\nEOF\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-configure-benchmark-providers",children:"Step 2: Configure Benchmark Providers"}),"\n",(0,s.jsxs)(n.p,{children:["Edit your ",(0,s.jsx)(n.code,{children:"trainloop.config.yaml"})," to specify which models to benchmark:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# trainloop.config.yaml\ntrainloop:\n  data_folder: "./data"\n  \n  # Benchmarking configuration\n  benchmark:\n    providers:\n      # OpenAI models\n      - provider: openai\n        model: gpt-4o\n        temperature: 0.7\n        max_tokens: 1000\n      \n      - provider: openai\n        model: gpt-4o-mini\n        temperature: 0.7\n        max_tokens: 1000\n      \n      # Anthropic models\n      - provider: anthropic\n        model: claude-3-5-sonnet-20241022\n        temperature: 0.7\n        max_tokens: 1000\n      \n      - provider: anthropic\n        model: claude-3-haiku-20240307\n        temperature: 0.7\n        max_tokens: 1000\n    \n    # Optional: Limit number of samples for faster benchmarking\n    max_samples: 100\n    \n    # Optional: Parallel execution settings\n    max_concurrent_requests: 5\n    \n    # Optional: Cost tracking\n    track_costs: true\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-run-benchmarking",children:"Step 3: Run Benchmarking"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Run benchmark against all configured providers\ntrainloop benchmark\n\n# Run benchmark for specific tag\ntrainloop benchmark --tag greeting-generation\n\n# Run benchmark with custom config\ntrainloop benchmark --config custom-benchmark.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"understanding-benchmark-results",children:"Understanding Benchmark Results"}),"\n",(0,s.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsx)(n.p,{children:"TrainLoop provides several performance metrics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\ud83d\udcca Benchmark Results Summary\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nModel Performance:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Model                           \u2502 Avg     \u2502 Min     \u2502 Max     \u2502 Samples \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai/gpt-4o                  \u2502 0.85    \u2502 0.67    \u2502 1.00    \u2502 100     \u2502\n\u2502 openai/gpt-4o-mini             \u2502 0.82    \u2502 0.60    \u2502 1.00    \u2502 100     \u2502\n\u2502 anthropic/claude-3-5-sonnet    \u2502 0.88    \u2502 0.73    \u2502 1.00    \u2502 100     \u2502\n\u2502 anthropic/claude-3-haiku       \u2502 0.79    \u2502 0.53    \u2502 1.00    \u2502 100     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nCost Analysis:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Model                           \u2502 Cost/1K tok \u2502 Total Cost  \u2502 Cost/Score  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai/gpt-4o                  \u2502 $0.015      \u2502 $4.50       \u2502 $0.053      \u2502\n\u2502 openai/gpt-4o-mini             \u2502 $0.001      \u2502 $0.30       \u2502 $0.004      \u2502\n\u2502 anthropic/claude-3-5-sonnet    \u2502 $0.015      \u2502 $4.80       \u2502 $0.055      \u2502\n\u2502 anthropic/claude-3-haiku       \u2502 $0.001      \u2502 $0.32       \u2502 $0.004      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"metric-level-analysis",children:"Metric-Level Analysis"}),"\n",(0,s.jsx)(n.p,{children:"View performance by individual metrics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Detailed breakdown by metric\ntrainloop benchmark --detailed\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\ud83d\udcc8 Detailed Metric Analysis\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nMetric: has_greeting_word\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Model                           \u2502 Score   \u2502 Samples \u2502 Pass %  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai/gpt-4o                  \u2502 0.95    \u2502 100     \u2502 95%     \u2502\n\u2502 openai/gpt-4o-mini             \u2502 0.93    \u2502 100     \u2502 93%     \u2502\n\u2502 anthropic/claude-3-5-sonnet    \u2502 0.97    \u2502 100     \u2502 97%     \u2502\n\u2502 anthropic/claude-3-haiku       \u2502 0.89    \u2502 100     \u2502 89%     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nMetric: is_helpful_response\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Model                           \u2502 Score   \u2502 Samples \u2502 Pass %  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai/gpt-4o                  \u2502 0.82    \u2502 100     \u2502 82%     \u2502\n\u2502 openai/gpt-4o-mini             \u2502 0.75    \u2502 100     \u2502 75%     \u2502\n\u2502 anthropic/claude-3-5-sonnet    \u2502 0.86    \u2502 100     \u2502 86%     \u2502\n\u2502 anthropic/claude-3-haiku       \u2502 0.71    \u2502 100     \u2502 71%     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"analyzing-results-in-studio-ui",children:"Analyzing Results in Studio UI"}),"\n",(0,s.jsx)(n.h3,{id:"launch-studio-for-benchmark-analysis",children:"Launch Studio for Benchmark Analysis"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"trainloop studio\n"})}),"\n",(0,s.jsx)(n.h3,{id:"key-views-for-benchmark-analysis",children:"Key Views for Benchmark Analysis"}),"\n",(0,s.jsx)(n.h4,{id:"1-model-comparison-dashboard",children:"1. Model Comparison Dashboard"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Side-by-side performance comparison"}),"\n",(0,s.jsx)(n.li,{children:"Cost vs. performance scatter plots"}),"\n",(0,s.jsx)(n.li,{children:"Metric breakdown by model"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-sample-level-analysis",children:"2. Sample-Level Analysis"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Individual responses from each model"}),"\n",(0,s.jsx)(n.li,{children:"Quality differences for the same prompt"}),"\n",(0,s.jsx)(n.li,{children:"Edge case identification"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-cost-analysis",children:"3. Cost Analysis"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Total cost projections"}),"\n",(0,s.jsx)(n.li,{children:"Cost per quality score"}),"\n",(0,s.jsx)(n.li,{children:"ROI calculations"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"advanced-benchmarking-strategies",children:"Advanced Benchmarking Strategies"}),"\n",(0,s.jsx)(n.h3,{id:"1-stratified-benchmarking",children:"1. Stratified Benchmarking"}),"\n",(0,s.jsx)(n.p,{children:"Test different types of prompts separately:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/stratified_benchmark.py\nfrom trainloop_cli.eval_core.helpers import tag\n\n# Benchmark simple questions\nsimple_questions = tag("simple-qa").check(\n    has_correct_answer,\n    is_concise,\n    is_clear\n)\n\n# Benchmark complex reasoning\ncomplex_reasoning = tag("complex-reasoning").check(\n    has_logical_flow,\n    addresses_all_aspects,\n    shows_depth\n)\n\n# Benchmark creative tasks\ncreative_tasks = tag("creative").check(\n    is_original,\n    is_engaging,\n    follows_constraints\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-domain-specific-benchmarking",children:"2. Domain-Specific Benchmarking"}),"\n",(0,s.jsx)(n.p,{children:"Create benchmarks for your specific domain:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/medical_benchmark.py\nfrom trainloop_cli.eval_core.helpers import tag\n\n# Medical information accuracy\nmedical_accuracy = tag("medical-info").check(\n    is_medically_accurate,\n    avoids_diagnosis,\n    recommends_professional_consultation,\n    uses_appropriate_disclaimers\n)\n\n# Medical communication quality\nmedical_communication = tag("medical-info").check(\n    is_accessible_language,\n    shows_empathy,\n    is_reassuring_but_realistic,\n    provides_actionable_advice\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-time-series-benchmarking",children:"3. Time-Series Benchmarking"}),"\n",(0,s.jsx)(n.p,{children:"Track performance over time:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Run benchmarks regularly\ntrainloop benchmark --tag time-series-test --output benchmark-$(date +%Y%m%d).json\n\n# Compare with previous results\ntrainloop benchmark --compare-with benchmark-20240101.json\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-selection-framework",children:"Model Selection Framework"}),"\n",(0,s.jsx)(n.h3,{id:"1-define-your-priorities",children:"1. Define Your Priorities"}),"\n",(0,s.jsx)(n.p,{children:"Create a scoring framework based on your priorities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example priority weights\npriorities = {\n    "accuracy": 0.4,      # 40% weight\n    "cost": 0.3,          # 30% weight\n    "speed": 0.2,         # 20% weight\n    "creativity": 0.1     # 10% weight\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-normalize-scores",children:"2. Normalize Scores"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def calculate_composite_score(benchmark_results, priorities):\n    \"\"\"Calculate composite score based on priorities\"\"\"\n    models = {}\n    \n    for model_name, results in benchmark_results.items():\n        # Normalize scores (0-1 scale)\n        accuracy_score = results['avg_metric_score']\n        cost_score = 1 - (results['cost_per_token'] / max_cost)  # Lower cost = higher score\n        speed_score = 1 - (results['avg_response_time'] / max_time)  # Faster = higher score\n        creativity_score = results['creativity_metric']\n        \n        # Calculate weighted composite score\n        composite_score = (\n            accuracy_score * priorities['accuracy'] +\n            cost_score * priorities['cost'] +\n            speed_score * priorities['speed'] +\n            creativity_score * priorities['creativity']\n        )\n        \n        models[model_name] = composite_score\n    \n    return models\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-decision-matrix",children:"3. Decision Matrix"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Accuracy"}),(0,s.jsx)(n.th,{children:"Cost"}),(0,s.jsx)(n.th,{children:"Speed"}),(0,s.jsx)(n.th,{children:"Creativity"}),(0,s.jsx)(n.th,{children:"Composite"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPT-4o"}),(0,s.jsx)(n.td,{children:"0.85"}),(0,s.jsx)(n.td,{children:"0.2"}),(0,s.jsx)(n.td,{children:"0.7"}),(0,s.jsx)(n.td,{children:"0.8"}),(0,s.jsx)(n.td,{children:"0.64"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPT-4o-mini"}),(0,s.jsx)(n.td,{children:"0.82"}),(0,s.jsx)(n.td,{children:"0.9"}),(0,s.jsx)(n.td,{children:"0.8"}),(0,s.jsx)(n.td,{children:"0.7"}),(0,s.jsx)(n.td,{children:"0.81"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Claude-3.5-Sonnet"}),(0,s.jsx)(n.td,{children:"0.88"}),(0,s.jsx)(n.td,{children:"0.2"}),(0,s.jsx)(n.td,{children:"0.6"}),(0,s.jsx)(n.td,{children:"0.9"}),(0,s.jsx)(n.td,{children:"0.66"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Claude-3-Haiku"}),(0,s.jsx)(n.td,{children:"0.79"}),(0,s.jsx)(n.td,{children:"0.9"}),(0,s.jsx)(n.td,{children:"0.9"}),(0,s.jsx)(n.td,{children:"0.6"}),(0,s.jsx)(n.td,{children:"0.79"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"production-benchmarking",children:"Production Benchmarking"}),"\n",(0,s.jsx)(n.h3,{id:"1-automated-benchmarking",children:"1. Automated Benchmarking"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Create benchmark automation script\ncat > benchmark_automation.sh << \'EOF\'\n#!/bin/bash\n\n# Run daily benchmarks\ntrainloop benchmark --tag daily-benchmark --output "benchmark-$(date +%Y%m%d).json"\n\n# Compare with baseline\ntrainloop benchmark --compare-with baseline-benchmark.json\n\n# Alert if performance drops\nif [[ $? -ne 0 ]]; then\n    echo "Performance regression detected!" | mail -s "Benchmark Alert" team@company.com\nfi\nEOF\n\n# Make executable\nchmod +x benchmark_automation.sh\n\n# Add to cron for daily execution\necho "0 2 * * * /path/to/benchmark_automation.sh" | crontab -\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-continuous-monitoring",children:"2. Continuous Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/continuous_monitoring.py\nimport datetime\nfrom trainloop_cli.eval_core.helpers import tag\n\n# Tag with timestamp for tracking\ncurrent_date = datetime.datetime.now().strftime("%Y-%m-%d")\nmonitoring_tag = f"continuous-monitoring-{current_date}"\n\n# Monitor key metrics\nmonitoring_results = tag(monitoring_tag).check(\n    core_functionality_works,\n    response_quality_maintained,\n    cost_within_budget,\n    response_time_acceptable\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-ab-testing-framework",children:"3. A/B Testing Framework"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# trainloop/eval/suites/ab_testing.py\nfrom trainloop_cli.eval_core.helpers import tag\n\n# Test new model against current production model\ndef ab_test_models(test_model, control_model, sample_size=1000):\n    \"\"\"\n    Run A/B test between two models\n    \"\"\"\n    # Run benchmark for both models\n    test_results = benchmark_model(test_model, sample_size)\n    control_results = benchmark_model(control_model, sample_size)\n    \n    # Statistical significance testing\n    significance = calculate_statistical_significance(test_results, control_results)\n    \n    return {\n        'test_model': test_model,\n        'control_model': control_model,\n        'test_performance': test_results['avg_score'],\n        'control_performance': control_results['avg_score'],\n        'improvement': test_results['avg_score'] - control_results['avg_score'],\n        'statistical_significance': significance,\n        'recommendation': 'deploy' if significance > 0.95 and test_results['avg_score'] > control_results['avg_score'] else 'keep_current'\n    }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"common-benchmarking-scenarios",children:"Common Benchmarking Scenarios"}),"\n",(0,s.jsx)(n.h3,{id:"1-cost-optimization",children:"1. Cost Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Low-cost benchmark configuration\ntrainloop:\n  benchmark:\n    providers:\n      - provider: openai\n        model: gpt-4o-mini\n      - provider: anthropic\n        model: claude-3-haiku-20240307\n      - provider: google\n        model: gemini-pro\n    \n    # Focus on cost-effective models\n    cost_threshold: 0.01  # Max $0.01 per 1K tokens\n    performance_threshold: 0.75  # Min 75% pass rate\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-accuracy-optimization",children:"2. Accuracy Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# High-accuracy benchmark configuration\ntrainloop:\n  benchmark:\n    providers:\n      - provider: openai\n        model: gpt-4o\n        temperature: 0.1  # Lower temperature for consistency\n      - provider: anthropic\n        model: claude-3-5-sonnet-20241022\n        temperature: 0.1\n    \n    # Focus on accuracy metrics\n    accuracy_weight: 0.8\n    cost_weight: 0.2\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-speed-optimization",children:"3. Speed Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Speed-focused benchmark configuration\ntrainloop:\n  benchmark:\n    providers:\n      - provider: openai\n        model: gpt-4o-mini\n        max_tokens: 500  # Shorter responses\n      - provider: anthropic\n        model: claude-3-haiku-20240307\n        max_tokens: 500\n    \n    # Measure response times\n    track_response_times: true\n    max_response_time: 3.0  # 3 second timeout\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"1-start-small-scale-up",children:"1. Start Small, Scale Up"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Start with small sample size\ntrainloop benchmark --max-samples 10\n\n# Increase gradually\ntrainloop benchmark --max-samples 100\n\n# Full benchmark when confident\ntrainloop benchmark\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-use-representative-data",children:"2. Use Representative Data"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Ensure your benchmark data represents real usage\ndef create_representative_benchmark():\n    """Create benchmark data that matches production patterns"""\n    \n    # Sample from different time periods\n    morning_samples = tag("morning-usage").sample(25)\n    afternoon_samples = tag("afternoon-usage").sample(25)\n    evening_samples = tag("evening-usage").sample(25)\n    weekend_samples = tag("weekend-usage").sample(25)\n    \n    # Combine for comprehensive benchmark\n    return morning_samples + afternoon_samples + evening_samples + weekend_samples\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-regular-benchmarking",children:"3. Regular Benchmarking"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Weekly performance check\n0 0 * * 1 trainloop benchmark --tag weekly-check\n\n# Monthly comprehensive benchmark\n0 0 1 * * trainloop benchmark --comprehensive\n\n# Quarterly model evaluation\n0 0 1 1,4,7,10 * trainloop benchmark --full-evaluation\n"})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsx)(n.h4,{id:"1-api-rate-limits",children:"1. API Rate Limits"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Reduce concurrent requests\ntrainloop benchmark --max-concurrent 2\n\n# Add delays between requests\ntrainloop benchmark --request-delay 1.0\n"})}),"\n",(0,s.jsx)(n.h4,{id:"2-inconsistent-results",children:"2. Inconsistent Results"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Use larger sample sizes\ntrainloop benchmark --max-samples 500\n\n# Lower temperature for consistency\ntrainloop benchmark --temperature 0.1\n"})}),"\n",(0,s.jsx)(n.h4,{id:"3-cost-concerns",children:"3. Cost Concerns"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Limit sample size\ntrainloop benchmark --max-samples 50\n\n# Use cheaper models for initial testing\ntrainloop benchmark --models gpt-4o-mini,claude-3-haiku\n"})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Congratulations! You now know how to benchmark and compare LLM models effectively. Continue with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/tutorials/production-setup",children:"Production Setup and CI/CD"})})," - Deploy evaluations in production environments"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Benchmark regularly"})," - Model performance changes over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Consider multiple factors"})," - Balance accuracy, cost, and speed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use representative data"})," - Ensure benchmarks match real usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automate the process"})," - Set up continuous benchmarking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Make data-driven decisions"})," - Replace intuition with evidence"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Ready to deploy your evaluations in production? Continue with ",(0,s.jsx)(n.a,{href:"/tutorials/production-setup",children:"Production Setup and CI/CD"}),"!"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var r=i(6540);const s={},a=r.createContext(s);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);