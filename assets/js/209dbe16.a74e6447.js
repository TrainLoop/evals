"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1484],{7736:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"tutorials/advanced-metrics","title":"Advanced Metrics with LLM Judge","description":"In this tutorial, you\'ll learn to use LLM Judge to create sophisticated evaluation metrics that go beyond simple rule-based checks. LLM Judge uses AI to evaluate AI, enabling complex quality assessments.","source":"@site/docs/tutorials/advanced-metrics.md","sourceDirName":"tutorials","slug":"/tutorials/advanced-metrics","permalink":"/tutorials/advanced-metrics","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/tutorials/advanced-metrics.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docsSidebar","previous":{"title":"Writing Your First Evaluation","permalink":"/tutorials/first-evaluation"},"next":{"title":"Benchmarking and Model Comparison","permalink":"/tutorials/benchmarking"}}');var t=i(4848),a=i(8453);const l={sidebar_position:4},r="Advanced Metrics with LLM Judge",o={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction to LLM Judge",id:"introduction-to-llm-judge",level:2},{value:"What is LLM Judge?",id:"what-is-llm-judge",level:3},{value:"When to Use LLM Judge",id:"when-to-use-llm-judge",level:3},{value:"Basic LLM Judge Usage",id:"basic-llm-judge-usage",level:2},{value:"The <code>assert_true</code> Function",id:"the-assert_true-function",level:3},{value:"How LLM Judge Works",id:"how-llm-judge-works",level:3},{value:"Advanced LLM Judge Patterns",id:"advanced-llm-judge-patterns",level:2},{value:"Context-Aware Evaluation",id:"context-aware-evaluation",level:3},{value:"Domain-Specific Evaluation",id:"domain-specific-evaluation",level:3},{value:"Multi-Criteria Evaluation",id:"multi-criteria-evaluation",level:3},{value:"Writing Effective Claims",id:"writing-effective-claims",level:2},{value:"Best Practices for Claims",id:"best-practices-for-claims",level:3},{value:"1. Be Specific and Detailed",id:"1-be-specific-and-detailed",level:4},{value:"2. Make Claims Mutually Exclusive",id:"2-make-claims-mutually-exclusive",level:4},{value:"3. Include Context When Relevant",id:"3-include-context-when-relevant",level:4},{value:"Complex Evaluation Scenarios",id:"complex-evaluation-scenarios",level:2},{value:"Evaluating Reasoning and Logic",id:"evaluating-reasoning-and-logic",level:3},{value:"Evaluating Creativity and Originality",id:"evaluating-creativity-and-originality",level:3},{value:"Evaluating Completeness",id:"evaluating-completeness",level:3},{value:"Combining LLM Judge with Traditional Metrics",id:"combining-llm-judge-with-traditional-metrics",level:2},{value:"Hybrid Evaluation Suite",id:"hybrid-evaluation-suite",level:3},{value:"Performance-Optimized Approach",id:"performance-optimized-approach",level:3},{value:"Configuration and Optimization",id:"configuration-and-optimization",level:2},{value:"Configuring LLM Judge",id:"configuring-llm-judge",level:3},{value:"Optimizing Performance",id:"optimizing-performance",level:3},{value:"1. Use Caching",id:"1-use-caching",level:4},{value:"2. Batch Similar Evaluations",id:"2-batch-similar-evaluations",level:4},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Test LLM Judge Metrics",id:"test-llm-judge-metrics",level:3},{value:"Validate Against Human Judgment",id:"validate-against-human-judgment",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:2},{value:"1. Start with Rule-Based, Add LLM Judge",id:"1-start-with-rule-based-add-llm-judge",level:3},{value:"2. Use Specific, Detailed Claims",id:"2-use-specific-detailed-claims",level:3},{value:"3. Monitor and Validate Results",id:"3-monitor-and-validate-results",level:3},{value:"Common Pitfalls and Solutions",id:"common-pitfalls-and-solutions",level:2},{value:"1. Inconsistent Results",id:"1-inconsistent-results",level:3},{value:"2. Slow Performance",id:"2-slow-performance",level:3},{value:"3. Unreliable Evaluation",id:"3-unreliable-evaluation",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"LLM Judge Calls Failing",id:"llm-judge-calls-failing",level:4},{value:"Unexpected Results",id:"unexpected-results",level:4},{value:"Performance Issues",id:"performance-issues",level:4}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"advanced-metrics-with-llm-judge",children:"Advanced Metrics with LLM Judge"})}),"\n",(0,t.jsx)(n.p,{children:"In this tutorial, you'll learn to use LLM Judge to create sophisticated evaluation metrics that go beyond simple rule-based checks. LLM Judge uses AI to evaluate AI, enabling complex quality assessments."}),"\n",(0,t.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How to use LLM Judge for complex evaluations"}),"\n",(0,t.jsx)(n.li,{children:"When to use LLM Judge vs. rule-based metrics"}),"\n",(0,t.jsx)(n.li,{children:"How to write effective claims for LLM Judge"}),"\n",(0,t.jsx)(n.li,{children:"Best practices for reliable LLM Judge metrics"}),"\n",(0,t.jsx)(n.li,{children:"How to combine LLM Judge with traditional metrics"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Completed ",(0,t.jsx)(n.a,{href:"/tutorials/first-evaluation",children:"Writing Your First Evaluation"})]}),"\n",(0,t.jsx)(n.li,{children:"Understanding of basic metrics and suites"}),"\n",(0,t.jsx)(n.li,{children:"API keys for LLM providers (OpenAI, Anthropic, etc.)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-llm-judge",children:"Introduction to LLM Judge"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-llm-judge",children:"What is LLM Judge?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LLM Judge"})," is a TrainLoop feature that uses large language models to evaluate the quality of LLM outputs. It's particularly useful for:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Subjective quality assessment"})," - Tone, helpfulness, clarity"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex reasoning evaluation"})," - Logical consistency, accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain-specific criteria"})," - Professional standards, style guidelines"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nuanced semantic understanding"})," - Intent matching, context awareness"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"when-to-use-llm-judge",children:"When to Use LLM Judge"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Use LLM Judge For"}),(0,t.jsx)(n.th,{children:"Use Rule-Based Metrics For"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Subjective quality (tone, helpfulness)"}),(0,t.jsx)(n.td,{children:"Objective criteria (length, format)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Complex reasoning evaluation"}),(0,t.jsx)(n.td,{children:"Simple pattern matching"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Domain-specific expertise"}),(0,t.jsx)(n.td,{children:"Universal standards"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Nuanced understanding"}),(0,t.jsx)(n.td,{children:"Performance-critical checks"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"basic-llm-judge-usage",children:"Basic LLM Judge Usage"}),"\n",(0,t.jsxs)(n.h3,{id:"the-assert_true-function",children:["The ",(0,t.jsx)(n.code,{children:"assert_true"})," Function"]}),"\n",(0,t.jsx)(n.p,{children:"The core LLM Judge function compares two claims:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from trainloop_cli.eval_core.judge import assert_true\n\ndef is_helpful_response(sample: Sample) -> int:\n    """Check if response is helpful using LLM Judge"""\n    response = sample.output.get("content", "")\n    \n    positive_claim = f"The response \'{response}\' is helpful and provides useful information."\n    negative_claim = f"The response \'{response}\' is not helpful and doesn\'t provide useful information."\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"how-llm-judge-works",children:"How LLM Judge Works"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Claim Generation"})," - You provide positive and negative claims"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM Evaluation"})," - Multiple LLMs evaluate which claim is more true"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Consensus Building"})," - Results are aggregated across multiple calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Binary Result"})," - Returns 1 if positive claim wins, 0 if negative claim wins"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-llm-judge-patterns",children:"Advanced LLM Judge Patterns"}),"\n",(0,t.jsx)(n.h3,{id:"context-aware-evaluation",children:"Context-Aware Evaluation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def matches_user_intent(sample: Sample) -> int:\n    """Check if response matches user\'s intent"""\n    user_message = ""\n    for msg in sample.input.get("messages", []):\n        if msg.get("role") == "user":\n            user_message = msg.get("content", "")\n            break\n    \n    response = sample.output.get("content", "")\n    \n    positive_claim = f"The response \'{response}\' directly addresses the user\'s request: \'{user_message}\'"\n    negative_claim = f"The response \'{response}\' does not address the user\'s request: \'{user_message}\'"\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"domain-specific-evaluation",children:"Domain-Specific Evaluation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def follows_medical_guidelines(sample: Sample) -> int:\n    """Check if response follows medical communication guidelines"""\n    response = sample.output.get("content", "")\n    \n    positive_claim = f"""The response \'{response}\' follows proper medical communication guidelines by:\n    - Being accurate and evidence-based\n    - Avoiding definitive diagnoses\n    - Recommending professional consultation when appropriate\n    - Using clear, accessible language"""\n    \n    negative_claim = f"""The response \'{response}\' violates medical communication guidelines by:\n    - Making unsupported claims\n    - Providing definitive diagnoses\n    - Failing to recommend professional consultation\n    - Using confusing or inappropriate language"""\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multi-criteria-evaluation",children:"Multi-Criteria Evaluation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def is_professional_customer_service(sample: Sample) -> int:\n    """Evaluate multiple aspects of customer service quality"""\n    response = sample.output.get("content", "")\n    \n    positive_claim = f"""The response \'{response}\' demonstrates excellent customer service by:\n    - Showing empathy and understanding\n    - Providing clear, actionable solutions\n    - Maintaining a professional yet friendly tone\n    - Being concise while being thorough"""\n    \n    negative_claim = f"""The response \'{response}\' demonstrates poor customer service by:\n    - Lacking empathy or understanding\n    - Providing unclear or unhelpful solutions\n    - Using inappropriate tone or language\n    - Being either too brief or overly verbose"""\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"writing-effective-claims",children:"Writing Effective Claims"}),"\n",(0,t.jsx)(n.h3,{id:"best-practices-for-claims",children:"Best Practices for Claims"}),"\n",(0,t.jsx)(n.h4,{id:"1-be-specific-and-detailed",children:"1. Be Specific and Detailed"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Good - Specific criteria\npositive_claim = f\"The response '{response}' is helpful because it provides specific steps, explains the reasoning, and offers alternatives.\"\n\n# Bad - Too vague\npositive_claim = f\"The response '{response}' is good.\"\n"})}),"\n",(0,t.jsx)(n.h4,{id:"2-make-claims-mutually-exclusive",children:"2. Make Claims Mutually Exclusive"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Good - Clear opposite claims\npositive_claim = f\"The response '{response}' is factually accurate and well-supported.\"\nnegative_claim = f\"The response '{response}' contains factual errors or unsupported claims.\"\n\n# Bad - Not mutually exclusive\npositive_claim = f\"The response '{response}' is accurate.\"\nnegative_claim = f\"The response '{response}' is confusing.\"\n"})}),"\n",(0,t.jsx)(n.h4,{id:"3-include-context-when-relevant",children:"3. Include Context When Relevant"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def appropriate_response_tone(sample: Sample) -> int:\n    """Check if response tone matches the context"""\n    user_message = ""\n    for msg in sample.input.get("messages", []):\n        if msg.get("role") == "user":\n            user_message = msg.get("content", "")\n            break\n    \n    response = sample.output.get("content", "")\n    \n    # Include context in claims\n    positive_claim = f"Given the user\'s message \'{user_message}\', the response \'{response}\' uses an appropriate tone that matches the context and user\'s emotional state."\n    negative_claim = f"Given the user\'s message \'{user_message}\', the response \'{response}\' uses an inappropriate tone that doesn\'t match the context or user\'s emotional state."\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"complex-evaluation-scenarios",children:"Complex Evaluation Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"evaluating-reasoning-and-logic",children:"Evaluating Reasoning and Logic"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def has_logical_reasoning(sample: Sample) -> int:\n    """Check if response demonstrates logical reasoning"""\n    response = sample.output.get("content", "")\n    \n    positive_claim = f"""The response \'{response}\' demonstrates clear logical reasoning by:\n    - Presenting information in a logical sequence\n    - Making valid inferences from given information\n    - Avoiding logical fallacies\n    - Drawing appropriate conclusions"""\n    \n    negative_claim = f"""The response \'{response}\' lacks logical reasoning by:\n    - Presenting information in a confusing order\n    - Making invalid inferences\n    - Containing logical fallacies\n    - Drawing inappropriate conclusions"""\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"evaluating-creativity-and-originality",children:"Evaluating Creativity and Originality"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def is_creative_response(sample: Sample) -> int:\n    """Check if response is creative and original"""\n    response = sample.output.get("content", "")\n    \n    positive_claim = f"""The response \'{response}\' demonstrates creativity by:\n    - Offering unique or novel perspectives\n    - Using imaginative language or examples\n    - Providing original insights or solutions\n    - Avoiding clich\xe9d or generic content"""\n    \n    negative_claim = f"""The response \'{response}\' lacks creativity by:\n    - Offering only conventional perspectives\n    - Using predictable language or examples\n    - Providing generic insights or solutions\n    - Relying on clich\xe9d or overused content"""\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"evaluating-completeness",children:"Evaluating Completeness"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def provides_complete_answer(sample: Sample) -> int:\n    """Check if response completely addresses the question"""\n    user_message = ""\n    for msg in sample.input.get("messages", []):\n        if msg.get("role") == "user":\n            user_message = msg.get("content", "")\n            break\n    \n    response = sample.output.get("content", "")\n    \n    positive_claim = f"""The response \'{response}\' provides a complete answer to the question \'{user_message}\' by:\n    - Addressing all parts of the question\n    - Providing sufficient detail and explanation\n    - Covering relevant aspects and considerations\n    - Offering actionable information where appropriate"""\n    \n    negative_claim = f"""The response \'{response}\' provides an incomplete answer to the question \'{user_message}\' by:\n    - Ignoring parts of the question\n    - Providing insufficient detail or explanation\n    - Missing relevant aspects or considerations\n    - Failing to offer actionable information"""\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"combining-llm-judge-with-traditional-metrics",children:"Combining LLM Judge with Traditional Metrics"}),"\n",(0,t.jsx)(n.h3,{id:"hybrid-evaluation-suite",children:"Hybrid Evaluation Suite"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/hybrid_evaluation.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.rule_based import has_greeting_word, appropriate_length, contains_contact_info\nfrom ..metrics.llm_judge import is_helpful_response, matches_user_intent, is_professional_tone\n\n# Combine rule-based and LLM Judge metrics\nresults = tag("customer-support").check(\n    # Fast rule-based checks\n    has_greeting_word,\n    appropriate_length,\n    contains_contact_info,\n    \n    # Sophisticated LLM Judge evaluations\n    is_helpful_response,\n    matches_user_intent,\n    is_professional_tone\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"performance-optimized-approach",children:"Performance-Optimized Approach"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def smart_evaluation_suite(sample: Sample) -> dict:\n    \"\"\"Use rule-based metrics first, LLM Judge for edge cases\"\"\"\n    results = {}\n    \n    # Fast rule-based checks first\n    results['has_greeting'] = has_greeting_word(sample)\n    results['appropriate_length'] = appropriate_length(sample)\n    \n    # Only use LLM Judge for complex cases\n    if results['has_greeting'] and results['appropriate_length']:\n        results['is_helpful'] = is_helpful_response(sample)\n        results['matches_intent'] = matches_user_intent(sample)\n    else:\n        # Skip expensive LLM Judge checks for obviously bad responses\n        results['is_helpful'] = 0\n        results['matches_intent'] = 0\n    \n    return results\n"})}),"\n",(0,t.jsx)(n.h2,{id:"configuration-and-optimization",children:"Configuration and Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"configuring-llm-judge",children:"Configuring LLM Judge"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# trainloop.config.yaml\ntrainloop:\n  judge:\n    # Models to use for evaluation\n    models:\n      - openai/gpt-4o\n      - anthropic/claude-3-sonnet-20240229\n      - openai/gpt-4o-mini\n    \n    # Number of calls per model per claim\n    calls_per_model_per_claim: 3\n    \n    # Temperature for consistency\n    temperature: 0.1\n    \n    # Maximum tokens for judge responses\n    max_tokens: 100\n    \n    # Timeout for judge calls\n    timeout: 30\n"})}),"\n",(0,t.jsx)(n.h3,{id:"optimizing-performance",children:"Optimizing Performance"}),"\n",(0,t.jsx)(n.h4,{id:"1-use-caching",children:"1. Use Caching"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef cached_helpfulness_check(response_content: str) -> int:\n    """Cache LLM Judge results for identical responses"""\n    positive_claim = f"The response \'{response_content}\' is helpful and informative."\n    negative_claim = f"The response \'{response_content}\' is unhelpful and uninformative."\n    \n    return assert_true(positive_claim, negative_claim)\n\ndef is_helpful_cached(sample: Sample) -> int:\n    response = sample.output.get("content", "")\n    return cached_helpfulness_check(response)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"2-batch-similar-evaluations",children:"2. Batch Similar Evaluations"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def batch_tone_evaluation(samples: List[Sample]) -> List[int]:\n    """Evaluate tone for multiple samples efficiently"""\n    results = []\n    \n    for sample in samples:\n        response = sample.output.get("content", "")\n        positive_claim = f"The response \'{response}\' has a professional and appropriate tone."\n        negative_claim = f"The response \'{response}\' has an unprofessional or inappropriate tone."\n        \n        result = assert_true(positive_claim, negative_claim)\n        results.append(result)\n    \n    return results\n'})}),"\n",(0,t.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"test-llm-judge-metrics",children:"Test LLM Judge Metrics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def test_llm_judge_consistency():\n    """Test that LLM Judge metrics are consistent"""\n    sample = Sample(\n        input={"messages": [{"role": "user", "content": "Hello, how are you?"}]},\n        output={"content": "Hello! I\'m doing well, thank you for asking. How can I help you today?"}\n    )\n    \n    # Run the same metric multiple times\n    results = []\n    for i in range(5):\n        result = is_helpful_response(sample)\n        results.append(result)\n    \n    # Check consistency (should be mostly the same)\n    consistency_rate = sum(results) / len(results)\n    print(f"Consistency rate: {consistency_rate}")\n    \n    # Should be either very high (>0.8) or very low (<0.2)\n    assert consistency_rate > 0.8 or consistency_rate < 0.2, "Inconsistent LLM Judge results"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"validate-against-human-judgment",children:"Validate Against Human Judgment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def validate_against_human_judgment():\n    """Compare LLM Judge results with human evaluations"""\n    test_cases = [\n        {\n            "sample": Sample(\n                input={"messages": [{"role": "user", "content": "Explain quantum computing"}]},\n                output={"content": "Quantum computing is complicated stuff with atoms and things."}\n            ),\n            "human_rating": 0  # Human says this is not helpful\n        },\n        {\n            "sample": Sample(\n                input={"messages": [{"role": "user", "content": "Explain quantum computing"}]},\n                output={"content": "Quantum computing uses quantum mechanical properties like superposition and entanglement to process information in fundamentally different ways than classical computers, potentially solving certain problems exponentially faster."}\n            ),\n            "human_rating": 1  # Human says this is helpful\n        }\n    ]\n    \n    agreement_count = 0\n    for test_case in test_cases:\n        llm_rating = is_helpful_response(test_case["sample"])\n        if llm_rating == test_case["human_rating"]:\n            agreement_count += 1\n    \n    agreement_rate = agreement_count / len(test_cases)\n    print(f"Agreement with human judgment: {agreement_rate:.2%}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,t.jsx)(n.h3,{id:"1-start-with-rule-based-add-llm-judge",children:"1. Start with Rule-Based, Add LLM Judge"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Good progression\ndef comprehensive_quality_check(sample: Sample) -> int:\n    # Quick rule-based elimination\n    if not has_minimum_length(sample):\n        return 0\n    \n    if not contains_required_elements(sample):\n        return 0\n    \n    # Sophisticated LLM Judge evaluation\n    return is_high_quality_response(sample)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-use-specific-detailed-claims",children:"2. Use Specific, Detailed Claims"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Good - Specific and actionable\npositive_claim = f\"The response '{response}' provides accurate, step-by-step instructions that are easy to follow and include necessary warnings.\"\n\n# Bad - Vague and subjective\npositive_claim = f\"The response '{response}' is good.\"\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-monitor-and-validate-results",children:"3. Monitor and Validate Results"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Add monitoring to your LLM Judge metrics\ndef monitored_helpfulness_check(sample: Sample) -> int:\n    result = is_helpful_response(sample)\n    \n    # Log for analysis\n    log_metric_result("helpfulness", result, sample.output.get("content", ""))\n    \n    return result\n'})}),"\n",(0,t.jsx)(n.h2,{id:"common-pitfalls-and-solutions",children:"Common Pitfalls and Solutions"}),"\n",(0,t.jsx)(n.h3,{id:"1-inconsistent-results",children:"1. Inconsistent Results"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Problem:"})," LLM Judge returns different results for the same input"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lower temperature in configuration"}),"\n",(0,t.jsx)(n.li,{children:"Use more specific claims"}),"\n",(0,t.jsx)(n.li,{children:"Increase number of calls per claim"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-slow-performance",children:"2. Slow Performance"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Problem:"})," LLM Judge metrics are too slow"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use caching for repeated content"}),"\n",(0,t.jsx)(n.li,{children:"Combine with rule-based pre-filtering"}),"\n",(0,t.jsx)(n.li,{children:"Use faster models for simple evaluations"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-unreliable-evaluation",children:"3. Unreliable Evaluation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Problem:"})," LLM Judge doesn't match human judgment"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Validate against human examples"}),"\n",(0,t.jsx)(n.li,{children:"Refine claim wording"}),"\n",(0,t.jsx)(n.li,{children:"Use multiple models for consensus"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"You now know how to create sophisticated evaluation metrics using LLM Judge! Continue with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/tutorials/benchmarking",children:"Benchmarking and Model Comparison"})})," - Compare different LLM providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/tutorials/production-setup",children:"Production Setup"})})," - Deploy evaluations in CI/CD pipelines"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,t.jsx)(n.h4,{id:"llm-judge-calls-failing",children:"LLM Judge Calls Failing"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check API keys and rate limits"}),"\n",(0,t.jsx)(n.li,{children:"Verify model names in configuration"}),"\n",(0,t.jsx)(n.li,{children:"Check network connectivity"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"unexpected-results",children:"Unexpected Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Review claim wording for clarity"}),"\n",(0,t.jsx)(n.li,{children:"Test with known examples"}),"\n",(0,t.jsx)(n.li,{children:"Check for model-specific biases"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement caching for repeated evaluations"}),"\n",(0,t.jsx)(n.li,{children:"Use rule-based pre-filtering"}),"\n",(0,t.jsx)(n.li,{children:"Consider using faster/cheaper models"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Ready to compare different LLM providers? Continue with ",(0,t.jsx)(n.a,{href:"/tutorials/benchmarking",children:"Benchmarking and Model Comparison"}),"!"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var s=i(6540);const t={},a=s.createContext(t);function l(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);