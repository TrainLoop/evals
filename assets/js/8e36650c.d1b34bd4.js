"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3929],{544:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/trainloop-evals-flow-f7503a03ef4a2d2befcf62d9afb22967.png"},3473:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"tutorials/getting-started","title":"Quick Start Guide","description":"Get up and running with TrainLoop Evals in under 5 minutes. This guide walks you through setting up your first evaluation project, collecting LLM data, and running your first evaluations.","source":"@site/docs/tutorials/getting-started.md","sourceDirName":"tutorials","slug":"/tutorials/getting-started","permalink":"/tutorials/getting-started","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/tutorials/getting-started.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Tutorials","permalink":"/tutorials/"},"next":{"title":"Writing Your First Evaluation","permalink":"/tutorials/first-evaluation"}}');var t=i(4848),r=i(8453);const a={sidebar_position:2},o="Quick Start Guide",l={},c=[{value:"What is TrainLoop Evals?",id:"what-is-trainloop-evals",level:2},{value:"How It Works",id:"how-it-works",level:3},{value:"Overview",id:"overview",level:2},{value:"Step 1: Create Your Workspace",id:"step-1-create-your-workspace",level:2},{value:"Why the trainloop/ Folder?",id:"why-the-trainloop-folder",level:3},{value:"Step 2: Prerequisites",id:"step-2-prerequisites",level:2},{value:"Step 3: Install and Configure SDK",id:"step-3-install-and-configure-sdk",level:2},{value:"Why Install the SDK?",id:"why-install-the-sdk",level:3},{value:"Choose Your Language",id:"choose-your-language",level:3},{value:"Python Application",id:"python-application",level:3},{value:"TypeScript/JavaScript Application",id:"typescriptjavascript-application",level:3},{value:"Go Application",id:"go-application",level:3},{value:"Step 4: Write Your First Metric",id:"step-4-write-your-first-metric",level:2},{value:"What are Metrics?",id:"what-are-metrics",level:3},{value:"What are Suites?",id:"what-are-suites",level:3},{value:"Step 5: Create Your First Test Suite",id:"step-5-create-your-first-test-suite",level:2},{value:"Step 6: Run Your First Evaluation",id:"step-6-run-your-first-evaluation",level:2},{value:"What Happens During Evaluation?",id:"what-happens-during-evaluation",level:3},{value:"What are Results/Verdicts?",id:"what-are-resultsverdicts",level:3},{value:"Step 7: Visualize Results",id:"step-7-visualize-results",level:2},{value:"What is Studio UI?",id:"what-is-studio-ui",level:3},{value:"Step 8: Iterate and Improve",id:"step-8-iterate-and-improve",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"\ud83d\udd27 Advanced Configuration",id:"-advanced-configuration",level:3},{value:"\ud83d\udcca Benchmarking",id:"-benchmarking",level:3},{value:"What Does Benchmarking Give You?",id:"what-does-benchmarking-give-you",level:4},{value:"\ud83c\udfaf Advanced Metrics",id:"-advanced-metrics",level:3},{value:"\ud83d\udd0d Registry System",id:"-registry-system",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"No data collected",id:"no-data-collected",level:4},{value:"Evaluation fails",id:"evaluation-fails",level:4},{value:"Studio UI doesn&#39;t show data",id:"studio-ui-doesnt-show-data",level:4},{value:"Getting Help",id:"getting-help",level:3},{value:"Congratulations! \ud83c\udf89",id:"congratulations-",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"quick-start-guide",children:"Quick Start Guide"})}),"\n",(0,t.jsx)(n.p,{children:"Get up and running with TrainLoop Evals in under 5 minutes. This guide walks you through setting up your first evaluation project, collecting LLM data, and running your first evaluations."}),"\n",(0,t.jsx)(n.h2,{id:"what-is-trainloop-evals",children:"What is TrainLoop Evals?"}),"\n",(0,t.jsx)(n.p,{children:"TrainLoop Evals is a complete evaluation framework for LLM applications that consists of three main components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SDKs"})," - Zero-touch instrumentation libraries that capture LLM request/response data from your application"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLI"})," - Command-line tool that analyzes your captured data using metrics and generates evaluation results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Studio UI"})," - Web interface for visualizing results, comparing models, and exploring your data"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"TrainLoop Evals Flow",src:i(544).A+"",width:"932",height:"1609"})}),"\n",(0,t.jsx)(n.p,{children:"The complete TrainLoop Evals workflow:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Your Application"})," (Python/TypeScript/Go) makes LLM calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TrainLoop SDK"})," automatically captures request/response data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Events"})," are stored as JSONL files in your data folder"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLI Tool"})," processes events using your custom metrics and suites"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluation Engine"})," applies metrics and generates results/benchmarks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Studio UI"})," provides interactive visualization and analysis"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"In this quick start, you'll:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create a workspace"})," - Set up the TrainLoop directory structure"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Install and configure SDKs"})," - Add automatic data collection to your LLM calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Write your first metric"})," - Create a simple evaluation function"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run evaluations"})," - Execute your first evaluation suite"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visualize results"})," - View results in the Studio UI"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-1-create-your-workspace",children:"Step 1: Create Your Workspace"}),"\n",(0,t.jsx)(n.p,{children:"First, create a new directory for your project and initialize TrainLoop:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir my-llm-project\ncd my-llm-project\ntrainloop init\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This creates the ",(0,t.jsx)(n.strong,{children:"trainloop/"})," folder structure that organizes your evaluation project:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"my-llm-project/\n\u251c\u2500\u2500 trainloop/                  # TrainLoop evaluation workspace\n\u2502   \u251c\u2500\u2500 data/                   # Data storage (auto-created)\n\u2502   \u2502   \u251c\u2500\u2500 events/             # Raw LLM interactions (.jsonl files)\n\u2502   \u2502   \u2514\u2500\u2500 results/            # Evaluation results (.json files)\n\u2502   \u251c\u2500\u2500 eval/                   # Your evaluation code\n\u2502   \u2502   \u251c\u2500\u2500 metrics/            # Custom metrics (Python functions)\n\u2502   \u2502   \u2514\u2500\u2500 suites/             # Test suites (groups of metrics)\n\u2502   \u2514\u2500\u2500 trainloop.config.yaml   # Configuration file\n\u2514\u2500\u2500 .gitignore                  # Pre-configured for TrainLoop\n"})}),"\n",(0,t.jsx)(n.h3,{id:"why-the-trainloop-folder",children:"Why the trainloop/ Folder?"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"trainloop/"})," folder is your ",(0,t.jsx)(n.strong,{children:"evaluation workspace"})," - separate from your application code. This separation allows you to:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Keep evaluation logic organized"})," - All metrics, suites, and data in one place"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Version control evaluations"})," - Track changes to your evaluation criteria"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Share evaluation setups"})," - Team members can use the same evaluation logic"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run evaluations anywhere"})," - Works with any application that produces LLM data"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-2-prerequisites",children:"Step 2: Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting, you'll need:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Keys"})," - For the LLM providers you want to evaluate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environment Setup"})," - Create a ",(0,t.jsx)(n.code,{children:".env"})," file in your project root:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Create .env file with your API keys\ncat > .env << 'EOF'\nOPENAI_API_KEY=your-openai-key-here\nANTHROPIC_API_KEY=your-anthropic-key-here\nEOF\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Folder Configuration"})," - Tell TrainLoop where to store data:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Set the data folder environment variable\nexport TRAINLOOP_DATA_FOLDER="$(pwd)/trainloop/data"\n\n# Or add it to your shell profile for persistence\necho \'export TRAINLOOP_DATA_FOLDER="$(pwd)/trainloop/data"\' >> ~/.bashrc\n'})}),"\n",(0,t.jsx)(n.h2,{id:"step-3-install-and-configure-sdk",children:"Step 3: Install and Configure SDK"}),"\n",(0,t.jsxs)(n.p,{children:["The TrainLoop SDK automatically captures LLM request/response data from your application with ",(0,t.jsx)(n.strong,{children:"zero code changes"}),". Here's how to install it for your language:"]}),"\n",(0,t.jsx)(n.h3,{id:"why-install-the-sdk",children:"Why Install the SDK?"}),"\n",(0,t.jsxs)(n.p,{children:["The SDK provides ",(0,t.jsx)(n.strong,{children:"automatic data collection"})," by:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intercepting LLM calls"})," - Captures requests and responses transparently"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Writing JSONL files"})," - Saves data to ",(0,t.jsx)(n.code,{children:"trainloop/data/events/"})," for analysis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adding metadata"})," - Includes timestamps, model info, and custom tags"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero performance impact"})," - Async logging that doesn't slow your app"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"choose-your-language",children:"Choose Your Language"}),"\n",(0,t.jsx)(n.p,{children:"Choose your application's language and follow the appropriate instructions:"}),"\n",(0,t.jsx)(n.h3,{id:"python-application",children:"Python Application"}),"\n",(0,t.jsx)(n.p,{children:"First, install the TrainLoop Python SDK:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install trainloop-llm-logging\n"})}),"\n",(0,t.jsx)(n.p,{children:"Create a simple Python script that makes LLM calls:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# app.py\nimport openai\nfrom trainloop_llm_logging import collect, trainloop_tag\n\n# Initialize TrainLoop collection - this patches OpenAI/Anthropic/etc. automatically\ncollect("trainloop/trainloop.config.yaml")\n\n# Set up OpenAI client (works with existing code!)\nclient = openai.OpenAI(api_key="your-api-key")\n\ndef generate_greeting(name):\n    """Generate a personalized greeting"""\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {"role": "system", "content": "You are a friendly assistant."},\n            {"role": "user", "content": f"Generate a warm greeting for {name}"}\n        ],\n        extra_headers=trainloop_tag("greeting-generation")  # Optional: tag for evaluation\n    )\n    return response.choices[0].message.content\n\n# Test the function\nif __name__ == "__main__":\n    greeting = generate_greeting("Alice")\n    print(f"Generated greeting: {greeting}")\n'})}),"\n",(0,t.jsx)(n.p,{children:"Run your application:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python app.py\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happens when you run this:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"The SDK automatically captures the OpenAI API call"}),"\n",(0,t.jsxs)(n.li,{children:["Request/response data is written to ",(0,t.jsx)(n.code,{children:"trainloop/data/events/YYYY-MM-DD.jsonl"})]}),"\n",(0,t.jsxs)(n.li,{children:["The tag ",(0,t.jsx)(n.code,{children:'"greeting-generation"'})," lets you filter this data during evaluation"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["\ud83d\udcda For complete Python SDK documentation, see ",(0,t.jsx)(n.a,{href:"/reference/sdk/python/api",children:"Python SDK API Reference"})]})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"typescriptjavascript-application",children:"TypeScript/JavaScript Application"}),"\n",(0,t.jsx)(n.p,{children:"First, install the TrainLoop TypeScript SDK:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"npm install trainloop-llm-logging\n"})}),"\n",(0,t.jsx)(n.p,{children:"Create a Node.js application:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-javascript",children:'// app.js\nconst { OpenAI } = require(\'openai\');\nconst { trainloopTag } = require(\'trainloop-llm-logging\');\n\n// Set up OpenAI client (works with existing code!)\nconst client = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nasync function generateGreeting(name) {\n  const response = await client.chat.completions.create({\n    model: "gpt-4o-mini",\n    messages: [\n      { role: "system", content: "You are a friendly assistant." },\n      { role: "user", content: `Generate a warm greeting for ${name}` }\n    ]\n  }, {\n    headers: { ...trainloopTag("greeting-generation") }  // Optional: tag for evaluation\n  });\n  \n  return response.choices[0].message.content;\n}\n\n// Test the function\ngenerateGreeting("Alice").then(greeting => {\n  console.log(`Generated greeting: ${greeting}`);\n});\n'})}),"\n",(0,t.jsx)(n.p,{children:"Run your application with automatic TrainLoop instrumentation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'TRAINLOOP_DATA_FOLDER=./trainloop/data NODE_OPTIONS="--require=trainloop-llm-logging" node app.js\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happens when you run this:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"--require=trainloop-llm-logging"})," flag automatically patches HTTP calls"]}),"\n",(0,t.jsxs)(n.li,{children:["LLM request/response data is written to ",(0,t.jsx)(n.code,{children:"trainloop/data/events/YYYY-MM-DD.jsonl"})]}),"\n",(0,t.jsxs)(n.li,{children:["The tag ",(0,t.jsx)(n.code,{children:'"greeting-generation"'})," lets you filter this data during evaluation"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["\ud83d\udcda For complete TypeScript SDK documentation, see ",(0,t.jsx)(n.a,{href:"/reference/sdk/typescript/api",children:"TypeScript SDK API Reference"})]})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"go-application",children:"Go Application"}),"\n",(0,t.jsx)(n.p,{children:"First, install the TrainLoop Go SDK:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"go get github.com/trainloop/evals/sdk/go/trainloop-llm-logging\n"})}),"\n",(0,t.jsx)(n.p,{children:"Create a Go application:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-go",children:'// main.go\npackage main\n\nimport (\n    "context"\n    "fmt"\n    "log"\n    "os"\n    \n    "github.com/sashabaranov/go-openai"\n    trainloop "github.com/trainloop/evals/sdk/go/trainloop-llm-logging"\n)\n\nfunc main() {\n    // Initialize TrainLoop - this wraps HTTP clients automatically\n    trainloop.Init()\n    defer trainloop.Shutdown()  // Ensure data is flushed\n    \n    // Set up OpenAI client (works with existing code!)\n    client := openai.NewClient(os.Getenv("OPENAI_API_KEY"))\n    \n    // Generate greeting\n    greeting := generateGreeting(client, "Alice")\n    fmt.Printf("Generated greeting: %s\\n", greeting)\n}\n\nfunc generateGreeting(client *openai.Client, name string) string {\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: openai.GPT4OMini,\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: "You are a friendly assistant."},\n                {Role: openai.ChatMessageRoleUser, Content: fmt.Sprintf("Generate a warm greeting for %s", name)},\n            },\n        },\n    )\n    \n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    return resp.Choices[0].Message.Content\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"Run your application:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"TRAINLOOP_DATA_FOLDER=./trainloop/data go run main.go\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happens when you run this:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"trainloop.Init()"})," wraps the HTTP client used by OpenAI"]}),"\n",(0,t.jsxs)(n.li,{children:["LLM request/response data is written to ",(0,t.jsx)(n.code,{children:"trainloop/data/events/YYYY-MM-DD.jsonl"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"trainloop.Shutdown()"})," ensures all data is flushed to disk"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["\ud83d\udcda For complete Go SDK documentation, see ",(0,t.jsx)(n.a,{href:"/reference/sdk/go/api",children:"Go SDK API Reference"})]})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-4-write-your-first-metric",children:"Step 4: Write Your First Metric"}),"\n",(0,t.jsx)(n.h3,{id:"what-are-metrics",children:"What are Metrics?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Metrics"})," are Python functions that evaluate your LLM outputs. They:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Take a ",(0,t.jsx)(n.code,{children:"Sample"})," (request/response pair) as input"]}),"\n",(0,t.jsxs)(n.li,{children:["Return ",(0,t.jsx)(n.code,{children:"1"})," for pass or ",(0,t.jsx)(n.code,{children:"0"})," for fail"]}),"\n",(0,t.jsx)(n.li,{children:"Define your evaluation criteria (accuracy, tone, format, etc.)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"what-are-suites",children:"What are Suites?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Suites"})," are groups of metrics that run together. They:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Organize related metrics"}),"\n",(0,t.jsx)(n.li,{children:'Filter data by tags (e.g., only evaluate "greeting-generation" calls)'}),"\n",(0,t.jsx)(n.li,{children:"Generate comprehensive evaluation reports"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Let's create metrics to evaluate greeting quality:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/metrics/greeting_quality.py\nfrom trainloop_cli.eval_core.types import Sample\n\ndef has_greeting_word(sample: Sample) -> int:\n    """Check if the response contains common greeting words"""\n    response_text = sample.output.get("content", "").lower()\n    greeting_words = ["hello", "hi", "greetings", "welcome", "good morning", "good afternoon", "good evening"]\n    \n    for word in greeting_words:\n        if word in response_text:\n            return 1  # Pass\n    return 0  # Fail\n\ndef is_personalized(sample: Sample) -> int:\n    """Check if the response appears to be personalized"""\n    response_text = sample.output.get("content", "").lower()\n    \n    # Get the user\'s input to find the name\n    user_message = ""\n    for msg in sample.input.get("messages", []):\n        if msg.get("role") == "user":\n            user_message = msg.get("content", "").lower()\n            break\n    \n    # Simple check: if user message contains a name and response contains it\n    if "alice" in user_message and "alice" in response_text:\n        return 1  # Pass\n    return 0  # Fail\n\ndef is_friendly_tone(sample: Sample) -> int:\n    """Check if the response has a friendly tone using LLM Judge"""\n    from trainloop_cli.eval_core.judge import assert_true\n    \n    response_text = sample.output.get("content", "")\n    \n    positive_claim = f"The response \'{response_text}\' has a warm and friendly tone."\n    negative_claim = f"The response \'{response_text}\' is cold or unfriendly."\n    \n    return assert_true(positive_claim, negative_claim)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"step-5-create-your-first-test-suite",children:"Step 5: Create Your First Test Suite"}),"\n",(0,t.jsx)(n.p,{children:"Create a test suite that uses your metrics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/greeting_evaluation.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.greeting_quality import has_greeting_word, is_personalized, is_friendly_tone\n\n# Test all greeting generation calls\nresults = tag("greeting-generation").check(\n    has_greeting_word,\n    is_personalized,\n    is_friendly_tone\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"step-6-run-your-first-evaluation",children:"Step 6: Run Your First Evaluation"}),"\n",(0,t.jsx)(n.h3,{id:"what-happens-during-evaluation",children:"What Happens During Evaluation?"}),"\n",(0,t.jsxs)(n.p,{children:["When you run ",(0,t.jsx)(n.code,{children:"trainloop eval"}),", the CLI:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reads event data"})," from ",(0,t.jsx)(n.code,{children:"trainloop/data/events/*.jsonl"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Applies metrics"})," to each LLM request/response pair"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generates verdicts"})," (pass/fail decisions) for each metric"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Saves results"})," to ",(0,t.jsx)(n.code,{children:"trainloop/data/results/*.json"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Execute your evaluation suite:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Run all evaluation suites\ntrainloop eval\n\n# Or run a specific suite\ntrainloop eval --suite greeting_evaluation\n"})}),"\n",(0,t.jsx)(n.p,{children:"You should see output like:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\ud83d\udd0d Discovering evaluation suites...\n\u2705 Found 1 suite: greeting_evaluation\n\n\ud83d\udcca Running evaluations...\n\u2705 greeting_evaluation: 3/3 metrics passed\n\n\ud83d\udcc8 Results saved to trainloop/data/results/\n"})}),"\n",(0,t.jsx)(n.h3,{id:"what-are-resultsverdicts",children:"What are Results/Verdicts?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Results"})," are the output of your evaluation containing:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verdicts"})," - Individual pass/fail decisions for each metric"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scores"})," - Aggregated metrics (% pass rate, averages, etc.)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metadata"})," - Timestamps, model info, tags, etc."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Raw data"})," - Original request/response pairs for debugging"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-7-visualize-results",children:"Step 7: Visualize Results"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-studio-ui",children:"What is Studio UI?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Studio UI"})," is a web interface that helps you:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explore evaluation results"})," - Interactive charts and tables"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Debug individual calls"})," - See exactly what your LLM said and why it passed/failed"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Track performance over time"})," - Monitor how metrics change across versions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compare models"})," - See which LLM performs best for your use case"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Launch the Studio UI to explore your results:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"trainloop studio\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This opens your browser to ",(0,t.jsx)(n.code,{children:"http://localhost:3000"})," where you can:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\ud83d\udcca ",(0,t.jsx)(n.strong,{children:"View evaluation results"})," in interactive charts and tables"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udccb ",(0,t.jsx)(n.strong,{children:"Browse individual LLM calls"})," and their evaluation verdicts"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udcc8 ",(0,t.jsx)(n.strong,{children:"Track metrics over time"})," across different runs"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,t.jsx)(n.strong,{children:"Filter and search"})," through your data by tags, models, dates"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udd27 ",(0,t.jsx)(n.strong,{children:"Debug failures"})," by seeing exact input/output pairs"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-8-iterate-and-improve",children:"Step 8: Iterate and Improve"}),"\n",(0,t.jsx)(n.p,{children:"Based on your evaluation results, you can:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Refine your prompts"})," - Improve system messages for better results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Add more metrics"})," - Create additional evaluation criteria"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test different models"})," - Compare performance across LLM providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automate evaluations"})," - Run evaluations in CI/CD pipelines"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Now that you have a working TrainLoop Evals setup, explore these advanced features:"}),"\n",(0,t.jsx)(n.h3,{id:"-advanced-configuration",children:"\ud83d\udd27 Advanced Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["Configure TrainLoop behavior in ",(0,t.jsx)(n.code,{children:"trainloop.config.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'trainloop:\n  data_folder: "./data"\n  log_level: "info"\n  \n  # LLM Judge configuration\n  judge:\n    models:\n      - openai/gpt-4o\n      - anthropic/claude-3-sonnet-20240229\n    calls_per_model_per_claim: 3\n    temperature: 0.7\n    \n  # Benchmarking configuration\n  benchmark:\n    providers:\n      - openai/gpt-4o\n      - openai/gpt-4o-mini\n      - anthropic/claude-3-5-sonnet-20241022\n    temperature: 0.7\n    max_tokens: 1000\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-benchmarking",children:"\ud83d\udcca Benchmarking"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Benchmarking"})," lets you compare multiple LLM providers on the same tasks to find the best model for your use case."]}),"\n",(0,t.jsx)(n.h4,{id:"what-does-benchmarking-give-you",children:"What Does Benchmarking Give You?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model comparison"})," - See which LLM performs best on your specific metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost analysis"})," - Compare performance vs. cost for different providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Objective evaluation"})," - Data-driven model selection instead of guesswork"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Regression testing"})," - Ensure new models don't hurt your performance"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Compare multiple LLM providers:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Add API keys to .env file\ncp trainloop/.env.example trainloop/.env\n# Edit trainloop/.env with your API keys\n\n# Run benchmarks\ntrainloop benchmark\n"})}),"\n",(0,t.jsx)(n.p,{children:"This will:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Re-run your prompts"})," against different LLM providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Apply your metrics"})," to each provider's responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generate comparison results"})," showing which model performs best"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Display results in Studio UI"})," with side-by-side comparisons"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-advanced-metrics",children:"\ud83c\udfaf Advanced Metrics"}),"\n",(0,t.jsx)(n.p,{children:"Create more sophisticated evaluation metrics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/metrics/advanced_metrics.py\nfrom trainloop_cli.eval_core.types import Sample\nfrom trainloop_cli.eval_core.judge import assert_true\nimport json\n\ndef response_length_appropriate(sample: Sample) -> int:\n    """Check if response length is appropriate for the task"""\n    response_text = sample.output.get("content", "")\n    word_count = len(response_text.split())\n    \n    # Greeting should be between 5-50 words\n    return 1 if 5 <= word_count <= 50 else 0\n\ndef follows_instructions(sample: Sample) -> int:\n    """Check if the response follows the given instructions"""\n    response_text = sample.output.get("content", "")\n    \n    # Use LLM Judge for complex evaluation\n    instruction_claim = f"The response \'{response_text}\' follows the instruction to generate a warm greeting."\n    violation_claim = f"The response \'{response_text}\' does not follow the instruction to generate a warm greeting."\n    \n    return assert_true(instruction_claim, violation_claim)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-registry-system",children:"\ud83d\udd0d Registry System"}),"\n",(0,t.jsx)(n.p,{children:"Add pre-built metrics and suites:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# List available components\ntrainloop add --list\n\n# Add a metric from the registry\ntrainloop add metric always_pass\n\n# Add a suite from the registry\ntrainloop add suite sample\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,t.jsx)(n.h4,{id:"no-data-collected",children:"No data collected"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Ensure ",(0,t.jsx)(n.code,{children:"TRAINLOOP_DATA_FOLDER"})," is set correctly"]}),"\n",(0,t.jsx)(n.li,{children:"Check that your LLM calls are being made"}),"\n",(0,t.jsx)(n.li,{children:"Verify the SDK is properly initialized"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"evaluation-fails",children:"Evaluation fails"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check that your metric functions return integers (0 or 1)"}),"\n",(0,t.jsxs)(n.li,{children:["Ensure the ",(0,t.jsx)(n.code,{children:"results"})," variable is defined in your suite files"]}),"\n",(0,t.jsx)(n.li,{children:"Verify your tag names match between data collection and evaluation"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"studio-ui-doesnt-show-data",children:"Studio UI doesn't show data"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Confirm evaluations have been run (",(0,t.jsx)(n.code,{children:"trainloop eval"}),")"]}),"\n",(0,t.jsxs)(n.li,{children:["Check that result files exist in ",(0,t.jsx)(n.code,{children:"trainloop/data/results/"})]}),"\n",(0,t.jsx)(n.li,{children:"Try refreshing the browser or restarting the Studio"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"getting-help",children:"Getting Help"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation"}),": Browse the ",(0,t.jsx)(n.a,{href:"../guides/",children:"guides"})," and ",(0,t.jsx)(n.a,{href:"../reference/",children:"reference"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discord"}),": Join our ",(0,t.jsx)(n.a,{href:"https://discord.gg/9NsEzwys",children:"community"})," for help and discussions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GitHub Issues"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/trainloop/evals/issues",children:"Report bugs or ask questions"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),": Check the ",(0,t.jsx)(n.a,{href:"https://github.com/trainloop/chat-ui-demo",children:"demo repository"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"congratulations-",children:"Congratulations! \ud83c\udf89"}),"\n",(0,t.jsx)(n.p,{children:"You've successfully set up TrainLoop Evals and created your first evaluation workflow. You're now ready to build robust, data-driven evaluations for your LLM applications."}),"\n",(0,t.jsxs)(n.p,{children:["Continue with the ",(0,t.jsx)(n.a,{href:"../guides/",children:"guides"})," to learn about advanced features, or explore the ",(0,t.jsx)(n.a,{href:"../reference/",children:"reference documentation"})," for detailed API information."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);