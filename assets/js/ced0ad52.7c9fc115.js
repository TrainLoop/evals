"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9001],{2701:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"llms","title":"LLM Cheatsheet","description":"Comprehensive cheatsheet for LLMs to help users implement TrainLoop evaluations","source":"@site/docs/llms.md","sourceDirName":".","slug":"/llms","permalink":"/llms","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/llms.md","tags":[],"version":"current","sidebarPosition":999,"frontMatter":{"title":"LLM Cheatsheet","description":"Comprehensive cheatsheet for LLMs to help users implement TrainLoop evaluations","sidebar_position":999},"sidebar":"docsSidebar","previous":{"title":"Testing Guide","permalink":"/development/testing"}}');var t=i(4848),l=i(8453);i(1028),i(6540),i(2303);const r={title:"LLM Cheatsheet",description:"Comprehensive cheatsheet for LLMs to help users implement TrainLoop evaluations",sidebar_position:999},a="LLM Cheatsheet - TrainLoop Evals",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Core Workflow: Collect \u2192 Evaluate \u2192 Compare \u2192 Visualize",id:"core-workflow-collect--evaluate--compare--visualize",level:2},{value:"1. Setup Phase",id:"1-setup-phase",level:2},{value:"Project Initialization",id:"project-initialization",level:3},{value:"2. SDK Integration &amp; Data Collection",id:"2-sdk-integration--data-collection",level:2},{value:"Multi-Language SDK Setup",id:"multi-language-sdk-setup",level:3},{value:"Python",id:"python",level:4},{value:"TypeScript/JavaScript",id:"typescriptjavascript",level:4},{value:"Zero-code-change collection",id:"zero-code-change-collection",level:5},{value:"Explicit collect function",id:"explicit-collect-function",level:5},{value:"Go (experimental)",id:"go-experimental",level:4},{value:"Key Integration Concepts",id:"key-integration-concepts",level:3},{value:"3. Evaluation System",id:"3-evaluation-system",level:2},{value:"Metrics: The Building Blocks",id:"metrics-the-building-blocks",level:3},{value:"Metric Structure Template",id:"metric-structure-template",level:4},{value:"Sample Object Properties",id:"sample-object-properties",level:4},{value:"Two Evaluation Approaches",id:"two-evaluation-approaches",level:3},{value:"1. Programmatic Evaluation (Fast, Deterministic)",id:"1-programmatic-evaluation-fast-deterministic",level:4},{value:"2. LLM Judge Evaluation (Flexible, Human-like)",id:"2-llm-judge-evaluation-flexible-human-like",level:4},{value:"Advanced Judge Configuration",id:"advanced-judge-configuration",level:4},{value:"Suites: Combining Metrics",id:"suites-combining-metrics",level:3},{value:"Suite Structure Template",id:"suite-structure-template",level:4},{value:"Example Suites",id:"example-suites",level:4},{value:"Advanced Suite Pattern: Custom Logic (Lower-Level API)",id:"advanced-suite-pattern-custom-logic-lower-level-api",level:4},{value:"Relationship: Metrics \u2194 Suites",id:"relationship-metrics--suites",level:3},{value:"4. Configuration &amp; Workflow",id:"4-configuration--workflow",level:2},{value:"Configuration File (<code>trainloop.config.yaml</code>)",id:"configuration-file-trainloopconfigyaml",level:3},{value:"Running Evaluations",id:"running-evaluations",level:3},{value:"Data Flow Example",id:"data-flow-example",level:3},{value:"5. Benchmarks: Comparing LLM Providers",id:"5-benchmarks-comparing-llm-providers",level:2},{value:"Purpose",id:"purpose",level:3},{value:"How Benchmarks Work",id:"how-benchmarks-work",level:3},{value:"Running Benchmarks",id:"running-benchmarks",level:3},{value:"Benchmark Results Structure",id:"benchmark-results-structure",level:3},{value:"Why Benchmarks Matter",id:"why-benchmarks-matter",level:3},{value:"6. Visualization: TrainLoop Studio",id:"6-visualization-trainloop-studio",level:2},{value:"Launching Studio",id:"launching-studio",level:3},{value:"What Studio Provides",id:"what-studio-provides",level:3},{value:"Key Studio Features",id:"key-studio-features",level:3},{value:"7. Complete End-to-End Example",id:"7-complete-end-to-end-example",level:2},{value:"Step 1: Initialize Project",id:"step-1-initialize-project",level:3},{value:"Step 2: Instrument Your Application",id:"step-2-instrument-your-application",level:3},{value:"Step 3: Create Evaluation Metrics",id:"step-3-create-evaluation-metrics",level:3},{value:"Step 4: Create Evaluation Suite",id:"step-4-create-evaluation-suite",level:3},{value:"Step 5: Run Evaluation",id:"step-5-run-evaluation",level:3},{value:"Step 6: Benchmark Providers",id:"step-6-benchmark-providers",level:3},{value:"Step 7: Visualize Results",id:"step-7-visualize-results",level:3},{value:"8. Best Practices &amp; Tips",id:"8-best-practices--tips",level:2},{value:"Metric Design",id:"metric-design",level:3},{value:"Suite Organization",id:"suite-organization",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Data Management",id:"data-management",level:3},{value:"9. Common Patterns &amp; Examples",id:"9-common-patterns--examples",level:2},{value:"Code Quality Evaluation",id:"code-quality-evaluation",level:3},{value:"Customer Service Quality",id:"customer-service-quality",level:3},{value:"Content Format Validation",id:"content-format-validation",level:3}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"llm-cheatsheet---trainloop-evals",children:"LLM Cheatsheet - TrainLoop Evals"})}),"\n",(0,t.jsx)(n.admonition,{title:"LLM Integration",type:"tip",children:(0,t.jsx)(n.p,{children:"This comprehensive cheatsheet contains everything an LLM needs to help users implement TrainLoop evaluations. Use the download (\ud83d\udce5) or copy (\ud83d\udccb) buttons in the breadcrumb area above to get the raw markdown."})}),"\n",(0,t.jsx)(n.p,{children:"This document provides everything an LLM needs to know to help users implement evaluations using TrainLoop's LLM evaluation framework."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"TrainLoop has created a comprehensive LLM evaluation framework that enables systematic testing, benchmarking, and quality assurance for AI applications. The framework supports Python, TypeScript, and Go applications with minimal code changes."}),"\n",(0,t.jsx)(n.h2,{id:"core-workflow-collect--evaluate--compare--visualize",children:"Core Workflow: Collect \u2192 Evaluate \u2192 Compare \u2192 Visualize"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collect"}),": SDKs automatically capture LLM interactions as structured data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluate"}),": Custom metrics test LLM outputs against specific criteria"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compare"}),": Benchmark the same prompts across different LLM providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visualize"}),": Studio UI provides interactive analysis and insights"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"1-setup-phase",children:"1. Setup Phase"}),"\n",(0,t.jsx)(n.h3,{id:"project-initialization",children:"Project Initialization"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),": ",(0,t.jsx)(n.code,{children:"trainloop init"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What it creates"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"trainloop/                    # Main evaluation directory\n\u251c\u2500\u2500 data/                     # Data storage (git-ignored)\n\u2502   \u251c\u2500\u2500 events/              # Raw JSONL files of LLM interactions\n\u2502   \u251c\u2500\u2500 results/             # Evaluation outcomes \n\u2502   \u251c\u2500\u2500 benchmarks/          # Provider comparison results\n\u2502   \u251c\u2500\u2500 judge_traces/        # LLM judge execution logs\n\u2502   \u2514\u2500\u2500 _registry.json       # Instrumentation tracking\n\u251c\u2500\u2500 eval/                    # Your evaluation logic\n\u2502   \u251c\u2500\u2500 metrics/            # Individual test functions\n\u2502   \u2514\u2500\u2500 suites/             # Collections of related tests\n\u251c\u2500\u2500 trainloop.config.yaml   # Configuration file\n\u2514\u2500\u2500 .venv/                  # Dedicated Python environment\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why this structure matters"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"data/"})}),": All raw and processed data lives here, git-ignored for privacy (this can also be a path to an S3 or GCS bucket)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"eval/"})}),": Your custom evaluation logic, version controlled"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configuration"}),": Centralized settings for judges, benchmarks, and data paths"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-sdk-integration--data-collection",children:"2. SDK Integration & Data Collection"}),"\n",(0,t.jsx)(n.h3,{id:"multi-language-sdk-setup",children:"Multi-Language SDK Setup"}),"\n",(0,t.jsx)(n.h4,{id:"python",children:"Python"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# At application startup - must happen BEFORE openai/anthropic imports\nfrom trainloop_llm_logging import collect, trainloop_tag\n\ncollect("../trainloop/trainloop.config.yaml", flush_immediately=True)\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n# Tag requests for targeted evaluation\nresponse = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[{"role": "user", "content": "Write a bubble sort function"}],\n    extra_headers=trainloop_tag("bubble-sort")  # Tags this request\n)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"typescriptjavascript",children:"TypeScript/JavaScript"}),"\n",(0,t.jsx)(n.h5,{id:"zero-code-change-collection",children:"Zero-code-change collection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Zero-code-change collection\nNODE_OPTIONS="--require=trainloop-llm-logging" npm run dev\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'// Tagged requests\nimport { trainloopTag } from "trainloop-llm-logging";\n\nconst response = await openai.chat.completions.create({\n    model: "gpt-4o",\n    messages: [{ role: "user", content: "Write a bubble sort function" }]\n}, {\n    headers: { ...trainloopTag("bubble-sort") }  // Tags this request\n});\n'})}),"\n",(0,t.jsx)(n.h5,{id:"explicit-collect-function",children:"Explicit collect function"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'// Tagged requests\nimport { trainloopTag, collect } from "trainloop-llm-logging";\n\ncollect(true)\n\nimport { OpenAI } from "openai";\n\nconst openai = new OpenAI();\n\nconst response = await openai.chat.completions.create({\n    model: "gpt-4o",\n    messages: [{ role: "user", content: "Write a bubble sort function" }]\n}, {\n    headers: { ...trainloopTag("bubble-sort") }  // Tags this request\n});\n'})}),"\n",(0,t.jsx)(n.h4,{id:"go-experimental",children:"Go (experimental)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-go",children:'// At main() startup\nimport "github.com/trainloop-ai/trainloop-llm-logging-go/trainloop"\n\nfunc main() {\n    trainloop.Collect()  // Auto-instruments HTTP transport\n    \n    // Tagged requests\n    req.Header.Add("X-TrainLoop-Tag", "bubble-sort")\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"key-integration-concepts",children:"Key Integration Concepts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-touch instrumentation"}),": SDKs automatically capture all LLM requests"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request tagging"}),": Use tags to categorize requests for targeted evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic storage"}),": Data flows to ",(0,t.jsx)(n.code,{children:"trainloop/data/events/"})," as JSONL files"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-provider support"}),": Works with OpenAI, Anthropic, Google, Cohere, etc."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-evaluation-system",children:"3. Evaluation System"}),"\n",(0,t.jsx)(n.h3,{id:"metrics-the-building-blocks",children:"Metrics: The Building Blocks"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Critical Rules for Metrics"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function Signature"}),": Must accept exactly one ",(0,t.jsx)(n.code,{children:"Sample"})," parameter"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Return Value"}),": Must return ",(0,t.jsx)(n.code,{children:"int"})," - either ",(0,t.jsx)(n.code,{children:"1"})," (pass) or ",(0,t.jsx)(n.code,{children:"0"})," (fail)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"File Naming"}),": Function name must match filename"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Import Required"}),": ",(0,t.jsx)(n.code,{children:"from trainloop_cli.eval_core.types import Sample"})]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"metric-structure-template",children:"Metric Structure Template"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: trainloop/eval/metrics/your_metric_name.py\nfrom trainloop_cli.eval_core.types import Sample\n\ndef your_metric_name(sample: Sample) -> int:\n    # Access LLM response\n    response = sample.output["content"]\n    \n    # Your evaluation logic here\n    if condition_met:\n        return 1  # Pass\n    else:\n        return 0  # Fail\n'})}),"\n",(0,t.jsx)(n.h4,{id:"sample-object-properties",children:"Sample Object Properties"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'sample.input          # List of conversation messages\nsample.output         # Dict with "content" key (LLM response)\nsample.model          # Model identifier (e.g., "openai/gpt-4o")\nsample.tag            # Request tag for categorization\nsample.duration_ms    # Response time in milliseconds\nsample.start_time_ms  # Request start timestamp\nsample.end_time_ms    # Request end timestamp\nsample.location       # Source code location info\n'})}),"\n",(0,t.jsx)(n.h3,{id:"two-evaluation-approaches",children:"Two Evaluation Approaches"}),"\n",(0,t.jsx)(n.h4,{id:"1-programmatic-evaluation-fast-deterministic",children:"1. Programmatic Evaluation (Fast, Deterministic)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def response_is_less_than_120_words(sample: Sample) -> int:\n    response = sample.output["content"]\n    word_count = len(response.split())\n    return 1 if word_count <= 120 else 0\n\ndef outputs_single_codeblock(sample: Sample) -> int:\n    content = sample.output["content"]\n    code_blocks = content.count("```")\n    return 1 if code_blocks == 2 else 0  # One opening, one closing\n'})}),"\n",(0,t.jsx)(n.h4,{id:"2-llm-judge-evaluation-flexible-human-like",children:"2. LLM Judge Evaluation (Flexible, Human-like)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from trainloop_cli.eval_core.judge import assert_true\n\ndef response_is_polite(sample: Sample) -> int:\n    response = sample.output["content"]\n    yes_claim = f"The reply \'{response}\' is polite, apologetic, and offers a clear resolution."\n    no_claim = f"The reply \'{response}\' is rude OR fails to apologize OR lacks a resolution."\n    return assert_true(yes_claim, no_claim)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"advanced-judge-configuration",children:"Advanced Judge Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def custom_evaluation(sample: Sample) -> int:\n    response = sample.output["content"]\n    \n    custom_config = {\n        "models": ["openai/gpt-4o", "anthropic/claude-3-sonnet"],\n        "calls_per_model_per_claim": 5,  # More calls = more reliable\n        "temperature": 0.3  # Lower = more consistent\n    }\n    \n    yes_claim = f"The response \'{response}\' meets our quality standards."\n    no_claim = f"The response \'{response}\' fails to meet our quality standards."\n    \n    return assert_true(yes_claim, no_claim, cfg=custom_config)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"suites-combining-metrics",children:"Suites: Combining Metrics"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Critical Rules for Suites"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tag-based filtering"}),": ",(0,t.jsx)(n.code,{children:'tag("your-tag")'})," selects which data to evaluate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metric combination"}),": ",(0,t.jsx)(n.code,{children:".check(metric1, metric2, ...)"})," applies multiple metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Required export"}),": Must export a ",(0,t.jsx)(n.code,{children:"results"})," variable"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"File naming"}),": Suite filename becomes result filename"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"suite-structure-template",children:"Suite Structure Template"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: trainloop/eval/suites/your_suite_name.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.metric1 import metric1\nfrom ..metrics.metric2 import metric2\n\n# This evaluates all samples tagged "your-tag" against both metrics\nresults = tag("your-tag").check(metric1, metric2)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"example-suites",children:"Example Suites"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Simple suite (single metric)\nresults = tag("active-voice").check(is_active_voice)\n\n# Multi-metric suite (comprehensive evaluation)\nresults = tag("code-generation").check(\n    outputs_single_codeblock,\n    code_runs_correctly\n)\n\n# Quality assurance suite\nresults = tag("customer-support").check(\n    response_is_polite,\n    response_is_less_than_120_words,\n    provides_clear_resolution\n)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"advanced-suite-pattern-custom-logic-lower-level-api",children:"Advanced Suite Pattern: Custom Logic (Lower-Level API)"}),"\n",(0,t.jsxs)(n.p,{children:["For complex evaluation scenarios requiring custom logic, filtering, or conditional evaluation, you can use the ",(0,t.jsx)(n.strong,{children:"lower-level API"})," instead of the standard ",(0,t.jsx)(n.code,{children:"tag().check()"})," pattern:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: trainloop/eval/suites/advanced_code_analysis.py\nfrom trainloop_cli.eval_core.types import Result, Sample\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics import does_compile, fcn_called_bubble_sort, is_readable\n\n# Get raw samples instead of using .check()\nsamples = tag("bubble-sort", raw=True)  # raw=True returns List[Sample]\nresults = []  # REQUIRED: must be named \'results\'\n\nfor sample in samples:\n    # Custom logic: only check readability if code compiles\n    compile_success = does_compile(sample)\n    results.append(Result(\n        metric="does_compile",\n        sample=sample,\n        passed=compile_success\n    ))\n    \n    # Conditional evaluation\n    if compile_success:\n        # Only check function name if compilation succeeds\n        bubble_sort_result = fcn_called_bubble_sort(sample)\n        results.append(Result(\n            metric="called_bubble_sort",\n            sample=sample,\n            passed=bubble_sort_result\n        ))\n        \n        # Additional readability check for working code\n        readable_result = is_readable(sample)\n        results.append(Result(\n            metric="code_readability",\n            sample=sample,\n            passed=readable_result\n        ))\n    else:\n        # Skip advanced metrics for non-compiling code\n        results.extend([\n            Result(metric="called_bubble_sort", sample=sample, passed=0),\n            Result(metric="code_readability", sample=sample, passed=0)\n        ])\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Differences from Standard Pattern"}),":"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"Standard Pattern"})}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"Lower-Level API"})})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'tag("name").check(metric1, metric2)'})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:'tag("name", raw=True)'})," + manual Result creation"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Declarative and concise"}),(0,t.jsx)(n.td,{children:"Imperative and flexible"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Automatic parallel execution"}),(0,t.jsx)(n.td,{children:"Sequential execution (manual parallelization possible)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"All metrics applied uniformly"}),(0,t.jsx)(n.td,{children:"Custom logic per sample"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"No filtering or conditional logic"}),(0,t.jsx)(n.td,{children:"Full control over evaluation flow"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"When to Use Lower-Level API"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional evaluation"}),": Only run certain metrics based on sample properties"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data preprocessing"}),": Transform samples before evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Custom filtering"}),": Skip samples based on specific criteria"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic metric selection"}),": Choose different metrics based on sample content"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex scoring"}),": Custom aggregation or weighting of results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance optimization"}),": Skip expensive metrics when possible"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"relationship-metrics--suites",children:"Relationship: Metrics \u2194 Suites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metrics"}),": Atomic evaluation functions (pure, reusable)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Suites"}),": Metric orchestrators (define what to test against which data)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tags"}),": The bridge linking application instrumentation to evaluation logic"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-configuration--workflow",children:"4. Configuration & Workflow"}),"\n",(0,t.jsxs)(n.h3,{id:"configuration-file-trainloopconfigyaml",children:["Configuration File (",(0,t.jsx)(n.code,{children:"trainloop.config.yaml"}),")"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Data management\ndata_folder: data\nflush_immediately: true\nlog_level: warn\n\n# LLM Judge settings (for assert_true)\njudge:\n  models:\n    - openai/gpt-4.1-2025-04-14\n    - anthropic/claude-sonnet-4-20250514\n  calls_per_model_per_claim: 3\n  temperature: 0.7\n  env_path: ../.env  # API keys location\n\n# Benchmark settings\nbenchmark:\n  providers:\n    - openai/gpt-4o\n    - anthropic/claude-sonnet-4-20250514\n    - gemini/gemini-2.5-flash\n  max_samples: 50\n  temperature: 0.7\n"})}),"\n",(0,t.jsx)(n.h3,{id:"running-evaluations",children:"Running Evaluations"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Run all evaluation suites\ntrainloop eval\n\n# Run specific suite only\ntrainloop eval --suite code_generation\n\n# Run with specific tag filter\ntrainloop eval --tag bubble-sort\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What happens during evaluation"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Reads JSONL files from ",(0,t.jsx)(n.code,{children:"data/events/"})]}),"\n",(0,t.jsx)(n.li,{children:"Filters samples by tag"}),"\n",(0,t.jsx)(n.li,{children:"Applies metrics"}),"\n",(0,t.jsxs)(n.li,{children:["Saves results to ",(0,t.jsx)(n.code,{children:"data/results/TIMESTAMP/SUITE_NAME.jsonl"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"data-flow-example",children:"Data Flow Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Application Code (with tags)\n    \u2193 [LLM requests]\ndata/events/1753146600572.jsonl    # Raw interactions\n    \u2193 [trainloop eval]\ndata/results/2025-07-21_15-44-19/  # Evaluation results\n\u251c\u2500\u2500 code_generation.jsonl          # Suite results\n\u2514\u2500\u2500 customer_support.jsonl\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"5-benchmarks-comparing-llm-providers",children:"5. Benchmarks: Comparing LLM Providers"}),"\n",(0,t.jsx)(n.h3,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:'Benchmarks answer: "Which LLM provider/model performs best for my specific use case?"'}),"\n",(0,t.jsx)(n.h3,{id:"how-benchmarks-work",children:"How Benchmarks Work"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Uses existing evaluation results"})," as baseline prompts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Re-runs identical prompts"})," through multiple LLM providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Applies same metrics"})," to new responses for fair comparison"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tracks performance metrics"}),": latency, cost, success rates"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"running-benchmarks",children:"Running Benchmarks"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"trainloop benchmark\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What it does"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Loads latest evaluation results from ",(0,t.jsx)(n.code,{children:"data/results/"})]}),"\n",(0,t.jsx)(n.li,{children:"Validates API keys for configured providers"}),"\n",(0,t.jsx)(n.li,{children:"Sends identical prompts to multiple providers"}),"\n",(0,t.jsx)(n.li,{children:"Applies existing metrics to all responses"}),"\n",(0,t.jsxs)(n.li,{children:["Saves comparative results to ",(0,t.jsx)(n.code,{children:"data/benchmarks/"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"benchmark-results-structure",children:"Benchmark Results Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-jsonl",children:'{"benchmark_config": {...}, "results": [...]}  # Header with config\n{"metric": "code_runs_correctly", "sample": {...}, "provider_result": {\n    "provider": "openai/gpt-4o",\n    "passed": 1,\n    "cost": 0.003,\n    "latency_ms": 1250,\n    "model": "gpt-4o"\n}}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"why-benchmarks-matter",children:"Why Benchmarks Matter"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance tracking"}),": Monitor which models work best over time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost optimization"}),": Find the best price/performance ratio"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Quality assurance"}),": Ensure consistency across different providers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Provider comparison"}),": Make data-driven decisions about LLM selection"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"6-visualization-trainloop-studio",children:"6. Visualization: TrainLoop Studio"}),"\n",(0,t.jsx)(n.h3,{id:"launching-studio",children:"Launching Studio"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"trainloop studio\n"})}),"\n",(0,t.jsx)(n.h3,{id:"what-studio-provides",children:"What Studio Provides"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interactive data exploration"})," using DuckDB queries"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual comparison"})," of evaluation results across time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Benchmark analysis"})," with performance metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Filtering and aggregation"})," by tags, models, time ranges"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Export capabilities"})," for further analysis"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-studio-features",children:"Key Studio Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"View evaluation trends over time"}),"\n",(0,t.jsx)(n.li,{children:"Compare provider performance side-by-side"}),"\n",(0,t.jsx)(n.li,{children:"Drill down into specific failures"}),"\n",(0,t.jsx)(n.li,{children:"Analyze cost and latency patterns"}),"\n",(0,t.jsx)(n.li,{children:"Export data for custom analysis"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"7-complete-end-to-end-example",children:"7. Complete End-to-End Example"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-initialize-project",children:"Step 1: Initialize Project"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"trainloop init\ncd trainloop\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-instrument-your-application",children:"Step 2: Instrument Your Application"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Python example\nfrom trainloop_llm_logging import collect, trainloop_tag\nfrom openai import OpenAI\n\ncollect("trainloop.config.yaml", flush_immediately=True)\nclient = OpenAI()\n\n# Tagged request for evaluation\nresponse = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[{"role": "user", "content": "Write a Python function that sorts a list"}],\n    extra_headers=trainloop_tag("code-generation")\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-create-evaluation-metrics",children:"Step 3: Create Evaluation Metrics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/metrics/code_runs_correctly.py\nfrom trainloop_cli.eval_core.types import Sample\nimport ast\n\ndef code_runs_correctly(sample: Sample) -> int:\n    content = sample.output["content"]\n    try:\n        # Extract and validate Python code\n        if "```python" in content:\n            code = content.split("```python")[1].split("```")[0].strip()\n            ast.parse(code)  # Check syntax\n            # Additional validation logic here\n            return 1\n    except:\n        return 0\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-create-evaluation-suite",children:"Step 4: Create Evaluation Suite"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/code_generation.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.code_runs_correctly import code_runs_correctly\nfrom ..metrics.outputs_single_codeblock import outputs_single_codeblock\n\nresults = tag("code-generation").check(\n    outputs_single_codeblock,\n    code_runs_correctly\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-run-evaluation",children:"Step 5: Run Evaluation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Generate some data first (run your application)\npython your_app.py\n\n# Then evaluate\ntrainloop eval\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-6-benchmark-providers",children:"Step 6: Benchmark Providers"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"trainloop benchmark\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-7-visualize-results",children:"Step 7: Visualize Results"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"trainloop studio\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"8-best-practices--tips",children:"8. Best Practices & Tips"}),"\n",(0,t.jsx)(n.h3,{id:"metric-design",children:"Metric Design"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Start simple"}),": Begin with programmatic checks before using LLM judges"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Combine approaches"}),": Use deterministic metrics for syntax, LLM judges for quality"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handle errors gracefully"}),": Always return 0 for exceptions unless specifically handling them"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Make judges specific"}),": Clear, detailed claims work better than vague ones"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"suite-organization",children:"Suite Organization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Group related metrics"}),": Code quality, customer service, factual accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use descriptive tags"}),": Make it easy to understand what's being tested"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Start small"}),": Begin with one metric per suite, expand gradually"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batch evaluations"}),": Run multiple metrics together for efficiency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use appropriate judge models"}),": Faster models for simple checks, stronger models for complex evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor costs"}),": LLM judges can be expensive at scale"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"data-management",children:"Data Management"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tag consistently"}),": Use the same tags across your application"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Regular cleanup"}),": Archive old evaluation results to manage disk space"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Version control eval/"}),": Keep your metrics and suites in git"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"9-common-patterns--examples",children:"9. Common Patterns & Examples"}),"\n",(0,t.jsx)(n.h3,{id:"code-quality-evaluation",children:"Code Quality Evaluation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Metrics for code evaluation\ndef outputs_single_codeblock(sample: Sample) -> int:\n    content = sample.output["content"]\n    return 1 if content.count("```") == 2 else 0\n\ndef code_compiles(sample: Sample) -> int:\n    # Extract and test code compilation\n    pass\n\ndef includes_comments(sample: Sample) -> int:\n    content = sample.output["content"]\n    return 1 if "#" in content else 0\n'})}),"\n",(0,t.jsx)(n.h3,{id:"customer-service-quality",children:"Customer Service Quality"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Metrics for customer support\ndef response_is_polite(sample: Sample) -> int:\n    response = sample.output["content"]\n    yes = f"The reply \'{response}\' is polite and professional."\n    no = f"The reply \'{response}\' is rude or unprofessional."\n    return assert_true(yes, no)\n\ndef provides_solution(sample: Sample) -> int:\n    response = sample.output["content"]\n    yes = f"The reply \'{response}\' offers a concrete solution or next steps."\n    no = f"The reply \'{response}\' doesn\'t provide actionable guidance."\n    return assert_true(yes, no)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"content-format-validation",children:"Content Format Validation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Metrics for content formatting\ndef proper_json_format(sample: Sample) -> int:\n    import json\n    try:\n        json.loads(sample.output["content"])\n        return 1\n    except:\n        return 0\n\ndef contains_required_sections(sample: Sample) -> int:\n    content = sample.output["content"].lower()\n    required = ["introduction", "methodology", "conclusion"]\n    return 1 if all(section in content for section in required) else 0\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"This cheatsheet covers everything needed to implement comprehensive LLM evaluation using TrainLoop's framework. Start with simple metrics, gradually add complexity, and use benchmarks to make data-driven decisions about your LLM implementation."})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);