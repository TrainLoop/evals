"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3184],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(6540);const a={},r=i.createContext(a);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},8851:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"tutorials/production-setup","title":"Production Setup and CI/CD","description":"In this tutorial, you\'ll learn how to deploy TrainLoop Evals in production environments, set up automated evaluation pipelines, and integrate with your development workflow.","source":"@site/docs/tutorials/production-setup.md","sourceDirName":"tutorials","slug":"/tutorials/production-setup","permalink":"/tutorials/production-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/tutorials/production-setup.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"docsSidebar","previous":{"title":"Benchmarking and Model Comparison","permalink":"/tutorials/benchmarking"},"next":{"title":"Guides","permalink":"/category/guides"}}');var a=t(4848),r=t(8453);const o={sidebar_position:6},s="Production Setup and CI/CD",l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Production Architecture Overview",id:"production-architecture-overview",level:2},{value:"Deployment Patterns",id:"deployment-patterns",level:3},{value:"1. Embedded Evaluation",id:"1-embedded-evaluation",level:4},{value:"2. Centralized Evaluation",id:"2-centralized-evaluation",level:4},{value:"3. Event-Driven Evaluation",id:"3-event-driven-evaluation",level:4},{value:"Setting Up Production Environment",id:"setting-up-production-environment",level:2},{value:"1. Environment Configuration",id:"1-environment-configuration",level:3},{value:"2. Docker Deployment",id:"2-docker-deployment",level:3},{value:"3. Kubernetes Deployment",id:"3-kubernetes-deployment",level:3},{value:"CI/CD Pipeline Integration",id:"cicd-pipeline-integration",level:2},{value:"1. GitHub Actions Workflow",id:"1-github-actions-workflow",level:3},{value:"2. Quality Gates Script",id:"2-quality-gates-script",level:3},{value:"3. GitLab CI Pipeline",id:"3-gitlab-ci-pipeline",level:3},{value:"Monitoring and Alerting",id:"monitoring-and-alerting",level:2},{value:"1. Prometheus Metrics",id:"1-prometheus-metrics",level:3},{value:"2. Custom Metrics Collection",id:"2-custom-metrics-collection",level:3},{value:"3. Grafana Dashboard",id:"3-grafana-dashboard",level:3},{value:"Scaling Considerations",id:"scaling-considerations",level:2},{value:"1. Horizontal Scaling",id:"1-horizontal-scaling",level:3},{value:"2. Data Partitioning",id:"2-data-partitioning",level:3},{value:"3. Distributed Evaluation",id:"3-distributed-evaluation",level:3},{value:"Security Best Practices",id:"security-best-practices",level:2},{value:"1. API Key Management",id:"1-api-key-management",level:3},{value:"2. Network Security",id:"2-network-security",level:3},{value:"3. Data Protection",id:"3-data-protection",level:3},{value:"Operational Runbooks",id:"operational-runbooks",level:2},{value:"1. Deployment Checklist",id:"1-deployment-checklist",level:3},{value:"2. Troubleshooting Guide",id:"2-troubleshooting-guide",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:2},{value:"1. Configuration Management",id:"1-configuration-management",level:3},{value:"2. Monitoring and Alerting",id:"2-monitoring-and-alerting",level:3},{value:"3. Performance Optimization",id:"3-performance-optimization",level:3},{value:"4. Security",id:"4-security",level:3},{value:"Congratulations! \ud83c\udf89",id:"congratulations-",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"production-setup-and-cicd",children:"Production Setup and CI/CD"})}),"\n",(0,a.jsx)(n.p,{children:"In this tutorial, you'll learn how to deploy TrainLoop Evals in production environments, set up automated evaluation pipelines, and integrate with your development workflow."}),"\n",(0,a.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"How to set up TrainLoop Evals in production environments"}),"\n",(0,a.jsx)(n.li,{children:"CI/CD pipeline integration patterns"}),"\n",(0,a.jsx)(n.li,{children:"Monitoring and alerting strategies"}),"\n",(0,a.jsx)(n.li,{children:"Scaling considerations for high-volume applications"}),"\n",(0,a.jsx)(n.li,{children:"Security best practices for production deployment"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Completed ",(0,a.jsx)(n.a,{href:"/tutorials/benchmarking",children:"Benchmarking and Model Comparison"})]}),"\n",(0,a.jsx)(n.li,{children:"Understanding of CI/CD concepts"}),"\n",(0,a.jsx)(n.li,{children:"Access to a CI/CD platform (GitHub Actions, GitLab CI, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Production environment access"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"production-architecture-overview",children:"Production Architecture Overview"}),"\n",(0,a.jsx)(n.h3,{id:"deployment-patterns",children:"Deployment Patterns"}),"\n",(0,a.jsx)(n.h4,{id:"1-embedded-evaluation",children:"1. Embedded Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Your Application \u2192 TrainLoop SDK \u2192 Event Data \u2192 Scheduled Evaluation\n"})}),"\n",(0,a.jsx)(n.p,{children:"Best for: Applications with moderate LLM usage"}),"\n",(0,a.jsx)(n.h4,{id:"2-centralized-evaluation",children:"2. Centralized Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Multiple Apps \u2192 Central Data Store \u2192 TrainLoop CLI \u2192 Results Dashboard\n"})}),"\n",(0,a.jsx)(n.p,{children:"Best for: Organizations with multiple LLM applications"}),"\n",(0,a.jsx)(n.h4,{id:"3-event-driven-evaluation",children:"3. Event-Driven Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Application \u2192 Event Queue \u2192 TrainLoop Worker \u2192 Real-time Results\n"})}),"\n",(0,a.jsx)(n.p,{children:"Best for: High-volume applications requiring immediate feedback"}),"\n",(0,a.jsx)(n.h2,{id:"setting-up-production-environment",children:"Setting Up Production Environment"}),"\n",(0,a.jsx)(n.h3,{id:"1-environment-configuration",children:"1. Environment Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Create production-ready configuration:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# trainloop.config.yaml (production)\ntrainloop:\n  data_folder: "/var/lib/trainloop/data"\n  log_level: "info"\n  \n  # Production data retention\n  data_retention:\n    events: 30  # Keep event data for 30 days\n    results: 90  # Keep results for 90 days\n  \n  # Performance settings\n  performance:\n    batch_size: 1000\n    max_concurrent_evaluations: 10\n    evaluation_timeout: 300  # 5 minutes\n  \n  # Judge configuration\n  judge:\n    models:\n      - openai/gpt-4o-mini  # Cost-effective for production\n      - anthropic/claude-3-haiku-20240307\n    calls_per_model_per_claim: 2  # Reduce for speed\n    temperature: 0.1  # Consistent results\n    timeout: 30\n  \n  # Monitoring\n  monitoring:\n    enabled: true\n    metrics_endpoint: "http://prometheus:9090"\n    alert_threshold: 0.7  # Alert if performance drops below 70%\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-docker-deployment",children:"2. Docker Deployment"}),"\n",(0,a.jsx)(n.p,{children:"Create production Docker setup:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile.production\nFROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create app user\nRUN useradd -m -s /bin/bash trainloop\nUSER trainloop\nWORKDIR /home/trainloop\n\n# Install TrainLoop CLI\nRUN pip install --user trainloop-cli\n\n# Copy configuration\nCOPY --chown=trainloop:trainloop trainloop.config.yaml .\nCOPY --chown=trainloop:trainloop evaluation/ ./evaluation/\n\n# Create data directory\nRUN mkdir -p data/events data/results\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:3000/health || exit 1\n\n# Default command\nCMD ["trainloop", "studio", "--host", "0.0.0.0", "--port", "3000"]\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# docker-compose.production.yml\nversion: \'3.8\'\n\nservices:\n  trainloop-evaluator:\n    build:\n      context: .\n      dockerfile: Dockerfile.production\n    environment:\n      - TRAINLOOP_DATA_FOLDER=/home/trainloop/data\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n    volumes:\n      - trainloop_data:/home/trainloop/data\n      - ./evaluation:/home/trainloop/evaluation:ro\n    ports:\n      - "3000:3000"\n    restart: unless-stopped\n    depends_on:\n      - prometheus\n      - grafana\n  \n  trainloop-scheduler:\n    build:\n      context: .\n      dockerfile: Dockerfile.production\n    environment:\n      - TRAINLOOP_DATA_FOLDER=/home/trainloop/data\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n    volumes:\n      - trainloop_data:/home/trainloop/data\n      - ./evaluation:/home/trainloop/evaluation:ro\n    command: ["sh", "-c", "while true; do trainloop eval && sleep 3600; done"]\n    restart: unless-stopped\n  \n  prometheus:\n    image: prom/prometheus:latest\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_data:/prometheus\n    ports:\n      - "9090:9090"\n    restart: unless-stopped\n  \n  grafana:\n    image: grafana/grafana:latest\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}\n    volumes:\n      - grafana_data:/var/lib/grafana\n      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards\n    ports:\n      - "3001:3000"\n    restart: unless-stopped\n\nvolumes:\n  trainloop_data:\n  prometheus_data:\n  grafana_data:\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-kubernetes-deployment",children:"3. Kubernetes Deployment"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# k8s/trainloop-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trainloop-evaluator\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: trainloop-evaluator\n  template:\n    metadata:\n      labels:\n        app: trainloop-evaluator\n    spec:\n      containers:\n      - name: trainloop\n        image: your-registry/trainloop-evaluator:latest\n        ports:\n        - containerPort: 3000\n        env:\n        - name: TRAINLOOP_DATA_FOLDER\n          value: "/data"\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: llm-api-keys\n              key: openai-api-key\n        - name: ANTHROPIC_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: llm-api-keys\n              key: anthropic-api-key\n        volumeMounts:\n        - name: data-volume\n          mountPath: /data\n        - name: config-volume\n          mountPath: /app/trainloop.config.yaml\n          subPath: trainloop.config.yaml\n        resources:\n          requests:\n            memory: "512Mi"\n            cpu: "250m"\n          limits:\n            memory: "1Gi"\n            cpu: "500m"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: trainloop-data-pvc\n      - name: config-volume\n        configMap:\n          name: trainloop-config\n'})}),"\n",(0,a.jsx)(n.h2,{id:"cicd-pipeline-integration",children:"CI/CD Pipeline Integration"}),"\n",(0,a.jsx)(n.h3,{id:"1-github-actions-workflow",children:"1. GitHub Actions Workflow"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# .github/workflows/evaluation-pipeline.yml\nname: LLM Evaluation Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n  schedule:\n    - cron: '0 */6 * * *'  # Run every 6 hours\n\njobs:\n  evaluate:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n    \n    - name: Install dependencies\n      run: |\n        pip install trainloop-cli\n        pip install -r requirements.txt\n    \n    - name: Set up TrainLoop\n      run: |\n        mkdir -p trainloop/data\n        export TRAINLOOP_DATA_FOLDER=\"$(pwd)/trainloop/data\"\n    \n    - name: Run application to collect data\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        TRAINLOOP_DATA_FOLDER: ${{ github.workspace }}/trainloop/data\n      run: |\n        # Run your application to generate test data\n        python scripts/generate_test_data.py\n    \n    - name: Run evaluations\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        TRAINLOOP_DATA_FOLDER: ${{ github.workspace }}/trainloop/data\n      run: |\n        trainloop eval --config trainloop.config.yaml\n    \n    - name: Run benchmarks\n      if: github.event_name == 'schedule'  # Only on scheduled runs\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        TRAINLOOP_DATA_FOLDER: ${{ github.workspace }}/trainloop/data\n      run: |\n        trainloop benchmark --max-samples 100\n    \n    - name: Check quality gates\n      run: |\n        python scripts/check_quality_gates.py\n    \n    - name: Upload results\n      uses: actions/upload-artifact@v3\n      with:\n        name: evaluation-results\n        path: trainloop/data/results/\n    \n    - name: Notify on failure\n      if: failure()\n      uses: 8398a7/action-slack@v3\n      with:\n        status: failure\n        webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n        text: \"LLM Evaluation Pipeline failed on ${{ github.ref }}\"\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-quality-gates-script",children:"2. Quality Gates Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# scripts/check_quality_gates.py\nimport json\nimport sys\nfrom pathlib import Path\n\ndef check_quality_gates():\n    \"\"\"Check if evaluation results meet quality thresholds\"\"\"\n    \n    # Define quality thresholds\n    thresholds = {\n        'minimum_pass_rate': 0.80,  # 80% of metrics must pass\n        'critical_metrics': {\n            'is_helpful': 0.85,\n            'is_accurate': 0.90,\n            'is_safe': 0.95\n        }\n    }\n    \n    # Load latest results\n    results_dir = Path('trainloop/data/results')\n    latest_result = max(results_dir.glob('*.json'), key=lambda p: p.stat().st_mtime)\n    \n    with open(latest_result) as f:\n        results = json.load(f)\n    \n    # Check overall pass rate\n    total_metrics = len(results['metrics'])\n    passed_metrics = sum(1 for metric in results['metrics'] if metric['passed'])\n    overall_pass_rate = passed_metrics / total_metrics\n    \n    if overall_pass_rate < thresholds['minimum_pass_rate']:\n        print(f\"\u274c Overall pass rate ({overall_pass_rate:.2%}) below threshold ({thresholds['minimum_pass_rate']:.2%})\")\n        sys.exit(1)\n    \n    # Check critical metrics\n    for metric_name, threshold in thresholds['critical_metrics'].items():\n        metric_result = next((m for m in results['metrics'] if m['name'] == metric_name), None)\n        if metric_result and metric_result['score'] < threshold:\n            print(f\"\u274c Critical metric {metric_name} ({metric_result['score']:.2%}) below threshold ({threshold:.2%})\")\n            sys.exit(1)\n    \n    print(\"\u2705 All quality gates passed!\")\n    return True\n\nif __name__ == \"__main__\":\n    check_quality_gates()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-gitlab-ci-pipeline",children:"3. GitLab CI Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# .gitlab-ci.yml\nstages:\n  - test\n  - evaluate\n  - benchmark\n  - deploy\n\nvariables:\n  TRAINLOOP_DATA_FOLDER: "${CI_PROJECT_DIR}/trainloop/data"\n\nbefore_script:\n  - pip install trainloop-cli\n  - mkdir -p trainloop/data\n\ntest:\n  stage: test\n  script:\n    - python -m pytest tests/\n  rules:\n    - if: $CI_PIPELINE_SOURCE == "merge_request_event"\n    - if: $CI_COMMIT_BRANCH == "main"\n\nevaluate:\n  stage: evaluate\n  script:\n    - python scripts/generate_test_data.py\n    - trainloop eval\n    - python scripts/check_quality_gates.py\n  artifacts:\n    paths:\n      - trainloop/data/results/\n    expire_in: 1 week\n  rules:\n    - if: $CI_PIPELINE_SOURCE == "merge_request_event"\n    - if: $CI_COMMIT_BRANCH == "main"\n\nbenchmark:\n  stage: benchmark\n  script:\n    - trainloop benchmark --max-samples 100\n  artifacts:\n    paths:\n      - trainloop/data/results/\n    expire_in: 1 month\n  rules:\n    - if: $CI_PIPELINE_SOURCE == "schedule"\n    - if: $CI_COMMIT_BRANCH == "main"\n  only:\n    - schedules\n\ndeploy:\n  stage: deploy\n  script:\n    - docker build -t trainloop-evaluator .\n    - docker push $CI_REGISTRY_IMAGE/trainloop-evaluator:latest\n    - kubectl apply -f k8s/\n  environment:\n    name: production\n  rules:\n    - if: $CI_COMMIT_BRANCH == "main"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"monitoring-and-alerting",children:"Monitoring and Alerting"}),"\n",(0,a.jsx)(n.h3,{id:"1-prometheus-metrics",children:"1. Prometheus Metrics"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'trainloop-evaluator'\n    static_configs:\n      - targets: ['trainloop-evaluator:3000']\n    scrape_interval: 30s\n    metrics_path: '/metrics'\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-custom-metrics-collection",children:"2. Custom Metrics Collection"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# scripts/collect_metrics.py\nimport json\nimport time\nfrom prometheus_client import start_http_server, Gauge, Counter, Histogram\n\n# Define metrics\nevaluation_score = Gauge('trainloop_evaluation_score', 'Overall evaluation score', ['suite'])\nevaluation_duration = Histogram('trainloop_evaluation_duration_seconds', 'Evaluation duration')\nevaluation_count = Counter('trainloop_evaluations_total', 'Total evaluations', ['status'])\nllm_api_calls = Counter('trainloop_llm_api_calls_total', 'LLM API calls', ['provider', 'model'])\n\ndef collect_and_report_metrics():\n    \"\"\"Collect TrainLoop metrics and report to Prometheus\"\"\"\n    \n    # Load latest results\n    results_dir = Path('trainloop/data/results')\n    latest_result = max(results_dir.glob('*.json'), key=lambda p: p.stat().st_mtime)\n    \n    with open(latest_result) as f:\n        results = json.load(f)\n    \n    # Report metrics\n    for suite_name, suite_results in results['suites'].items():\n        evaluation_score.labels(suite=suite_name).set(suite_results['score'])\n    \n    evaluation_duration.observe(results['duration'])\n    evaluation_count.labels(status='success').inc()\n    \n    # Report LLM API usage\n    for call in results['api_calls']:\n        llm_api_calls.labels(\n            provider=call['provider'],\n            model=call['model']\n        ).inc()\n\nif __name__ == \"__main__\":\n    start_http_server(8000)\n    \n    while True:\n        try:\n            collect_and_report_metrics()\n            time.sleep(60)  # Report every minute\n        except Exception as e:\n            print(f\"Error collecting metrics: {e}\")\n            evaluation_count.labels(status='error').inc()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-grafana-dashboard",children:"3. Grafana Dashboard"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "dashboard": {\n    "title": "TrainLoop Evals Dashboard",\n    "panels": [\n      {\n        "title": "Overall Evaluation Score",\n        "type": "stat",\n        "targets": [\n          {\n            "expr": "avg(trainloop_evaluation_score)",\n            "legendFormat": "Average Score"\n          }\n        ]\n      },\n      {\n        "title": "Evaluation Scores by Suite",\n        "type": "timeseries",\n        "targets": [\n          {\n            "expr": "trainloop_evaluation_score",\n            "legendFormat": "{{suite}}"\n          }\n        ]\n      },\n      {\n        "title": "Evaluation Duration",\n        "type": "timeseries",\n        "targets": [\n          {\n            "expr": "rate(trainloop_evaluation_duration_seconds_sum[5m]) / rate(trainloop_evaluation_duration_seconds_count[5m])",\n            "legendFormat": "Average Duration"\n          }\n        ]\n      },\n      {\n        "title": "API Usage by Provider",\n        "type": "piechart",\n        "targets": [\n          {\n            "expr": "sum by (provider) (trainloop_llm_api_calls_total)",\n            "legendFormat": "{{provider}}"\n          }\n        ]\n      }\n    ]\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"scaling-considerations",children:"Scaling Considerations"}),"\n",(0,a.jsx)(n.h3,{id:"1-horizontal-scaling",children:"1. Horizontal Scaling"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/trainloop-hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: trainloop-evaluator-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: trainloop-evaluator\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-data-partitioning",children:"2. Data Partitioning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# scripts/partition_data.py\nimport os\nfrom datetime import datetime, timedelta\n\ndef partition_data_by_date():\n    """Partition event data by date for better performance"""\n    \n    data_dir = Path(os.environ[\'TRAINLOOP_DATA_FOLDER\'])\n    events_dir = data_dir / \'events\'\n    \n    # Create date-based directories\n    for event_file in events_dir.glob(\'*.jsonl\'):\n        file_date = datetime.fromtimestamp(event_file.stat().st_mtime)\n        date_dir = events_dir / file_date.strftime(\'%Y/%m/%d\')\n        date_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Move file to date directory\n        event_file.rename(date_dir / event_file.name)\n\ndef cleanup_old_data():\n    """Remove old data based on retention policy"""\n    \n    data_dir = Path(os.environ[\'TRAINLOOP_DATA_FOLDER\'])\n    retention_days = 30\n    cutoff_date = datetime.now() - timedelta(days=retention_days)\n    \n    for old_file in data_dir.rglob(\'*.jsonl\'):\n        if datetime.fromtimestamp(old_file.stat().st_mtime) < cutoff_date:\n            old_file.unlink()\n            print(f"Removed old file: {old_file}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-distributed-evaluation",children:"3. Distributed Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# scripts/distributed_evaluation.py\nimport multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor\nfrom pathlib import Path\n\ndef evaluate_partition(partition_files):\n    \"\"\"Evaluate a partition of data files\"\"\"\n    # Set up separate data folder for this partition\n    partition_data_folder = f\"/tmp/trainloop_partition_{mp.current_process().pid}\"\n    os.environ['TRAINLOOP_DATA_FOLDER'] = partition_data_folder\n    \n    # Copy files to partition folder\n    for file in partition_files:\n        shutil.copy(file, partition_data_folder)\n    \n    # Run evaluation\n    subprocess.run(['trainloop', 'eval'], check=True)\n    \n    # Return results\n    results_dir = Path(partition_data_folder) / 'results'\n    return list(results_dir.glob('*.json'))\n\ndef distributed_evaluation():\n    \"\"\"Run evaluation across multiple processes\"\"\"\n    \n    # Get all event files\n    data_dir = Path(os.environ['TRAINLOOP_DATA_FOLDER'])\n    event_files = list((data_dir / 'events').glob('*.jsonl'))\n    \n    # Partition files\n    num_partitions = mp.cpu_count()\n    partitions = [event_files[i::num_partitions] for i in range(num_partitions)]\n    \n    # Run evaluation in parallel\n    with ProcessPoolExecutor(max_workers=num_partitions) as executor:\n        results = list(executor.map(evaluate_partition, partitions))\n    \n    # Combine results\n    combined_results = combine_evaluation_results(results)\n    return combined_results\n"})}),"\n",(0,a.jsx)(n.h2,{id:"security-best-practices",children:"Security Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"1-api-key-management",children:"1. API Key Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# k8s/secrets.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: llm-api-keys\ntype: Opaque\nstringData:\n  openai-api-key: "your-openai-key"\n  anthropic-api-key: "your-anthropic-key"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-network-security",children:"2. Network Security"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/network-policy.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: trainloop-network-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: trainloop-evaluator\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    ports:\n    - protocol: TCP\n      port: 3000\n  egress:\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 443  # HTTPS for API calls\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 53   # DNS\n  - to: []\n    ports:\n    - protocol: UDP\n      port: 53   # DNS\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-data-protection",children:"3. Data Protection"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# scripts/secure_data.py\nimport os\nimport hashlib\nfrom cryptography.fernet import Fernet\n\ndef encrypt_sensitive_data():\n    \"\"\"Encrypt sensitive data in event files\"\"\"\n    \n    key = os.environ.get('ENCRYPTION_KEY', Fernet.generate_key())\n    cipher = Fernet(key)\n    \n    data_dir = Path(os.environ['TRAINLOOP_DATA_FOLDER'])\n    \n    for event_file in data_dir.glob('events/*.jsonl'):\n        # Read, encrypt, and write back\n        with open(event_file, 'rb') as f:\n            encrypted_data = cipher.encrypt(f.read())\n        \n        with open(event_file, 'wb') as f:\n            f.write(encrypted_data)\n\ndef anonymize_data():\n    \"\"\"Remove or hash personally identifiable information\"\"\"\n    \n    data_dir = Path(os.environ['TRAINLOOP_DATA_FOLDER'])\n    \n    for event_file in data_dir.glob('events/*.jsonl'):\n        anonymized_lines = []\n        \n        with open(event_file, 'r') as f:\n            for line in f:\n                event = json.loads(line)\n                \n                # Hash user IDs\n                if 'user_id' in event:\n                    event['user_id'] = hashlib.sha256(event['user_id'].encode()).hexdigest()[:8]\n                \n                # Remove IP addresses\n                if 'ip_address' in event:\n                    del event['ip_address']\n                \n                anonymized_lines.append(json.dumps(event))\n        \n        with open(event_file, 'w') as f:\n            f.write('\\n'.join(anonymized_lines))\n"})}),"\n",(0,a.jsx)(n.h2,{id:"operational-runbooks",children:"Operational Runbooks"}),"\n",(0,a.jsx)(n.h3,{id:"1-deployment-checklist",children:"1. Deployment Checklist"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-markdown",children:"# TrainLoop Evals Deployment Checklist\n\n## Pre-Deployment\n- [ ] API keys are configured and valid\n- [ ] Configuration files are reviewed and approved\n- [ ] Resource limits are set appropriately\n- [ ] Monitoring and alerting are configured\n- [ ] Backup strategy is in place\n\n## Deployment\n- [ ] Run canary deployment first\n- [ ] Monitor system health during deployment\n- [ ] Verify evaluation pipeline is running\n- [ ] Check data collection is working\n- [ ] Validate dashboard access\n\n## Post-Deployment\n- [ ] Run smoke tests\n- [ ] Monitor performance metrics\n- [ ] Check log files for errors\n- [ ] Verify alerting is working\n- [ ] Update documentation\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-troubleshooting-guide",children:"2. Troubleshooting Guide"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-markdown",children:"# TrainLoop Evals Troubleshooting Guide\n\n## Common Issues\n\n### Evaluation Pipeline Failures\n**Symptoms:** Evaluations not running or failing\n**Causes:** API key issues, configuration errors, resource constraints\n**Solutions:**\n1. Check API key validity\n2. Verify configuration syntax\n3. Check resource utilization\n4. Review error logs\n\n### Data Collection Issues\n**Symptoms:** No event data being collected\n**Causes:** SDK not initialized, wrong data folder, network issues\n**Solutions:**\n1. Verify SDK initialization\n2. Check TRAINLOOP_DATA_FOLDER environment variable\n3. Test network connectivity\n4. Review application logs\n\n### Performance Issues\n**Symptoms:** Slow evaluation, high resource usage\n**Causes:** Large datasets, inefficient metrics, resource constraints\n**Solutions:**\n1. Optimize metrics for performance\n2. Implement data partitioning\n3. Scale resources horizontally\n4. Use caching where appropriate\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,a.jsx)(n.h3,{id:"1-configuration-management",children:"1. Configuration Management"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use environment-specific configurations"}),"\n",(0,a.jsx)(n.li,{children:"Store sensitive data in secure secret management"}),"\n",(0,a.jsx)(n.li,{children:"Version control all configuration files"}),"\n",(0,a.jsx)(n.li,{children:"Document configuration changes"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-monitoring-and-alerting",children:"2. Monitoring and Alerting"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Set up comprehensive monitoring"}),"\n",(0,a.jsx)(n.li,{children:"Define appropriate alert thresholds"}),"\n",(0,a.jsx)(n.li,{children:"Use structured logging"}),"\n",(0,a.jsx)(n.li,{children:"Implement health checks"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-performance-optimization",children:"3. Performance Optimization"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Partition large datasets"}),"\n",(0,a.jsx)(n.li,{children:"Use efficient metrics"}),"\n",(0,a.jsx)(n.li,{children:"Implement caching strategies"}),"\n",(0,a.jsx)(n.li,{children:"Monitor resource utilization"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-security",children:"4. Security"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Encrypt sensitive data"}),"\n",(0,a.jsx)(n.li,{children:"Use secure secret management"}),"\n",(0,a.jsx)(n.li,{children:"Implement network security policies"}),"\n",(0,a.jsx)(n.li,{children:"Regular security audits"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"congratulations-",children:"Congratulations! \ud83c\udf89"}),"\n",(0,a.jsx)(n.p,{children:"You've completed the TrainLoop Evals tutorial series! You now have the knowledge to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Set up comprehensive LLM evaluation systems"}),"\n",(0,a.jsx)(n.li,{children:"Write effective metrics and test suites"}),"\n",(0,a.jsx)(n.li,{children:"Use advanced features like LLM Judge and benchmarking"}),"\n",(0,a.jsx)(n.li,{children:"Deploy and monitor evaluations in production"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Explore the ",(0,a.jsx)(n.a,{href:"../guides/",children:"guides"})," for specific implementation patterns"]}),"\n",(0,a.jsxs)(n.li,{children:["Check the ",(0,a.jsx)(n.a,{href:"/reference/",children:"reference documentation"})," for detailed API information"]}),"\n",(0,a.jsx)(n.li,{children:"Join the community and share your experiences"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Keep evaluating, keep improving! \ud83d\ude80"})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);