"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1567],{5226:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"Introduction to TrainLoop Evals","href":"/intro","docId":"intro","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Installation","href":"/getting-started/installation","docId":"getting-started/installation","unlisted":false},{"type":"link","label":"Quick Start Guide","href":"/getting-started/quick-start","docId":"getting-started/quick-start","unlisted":false}],"href":"/category/getting-started"},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"How-to Guides","href":"/guides/","docId":"guides/index","unlisted":false}],"href":"/category/guides"},{"type":"category","label":"Reference","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"CLI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"trainloop init","href":"/reference/cli/init","docId":"reference/cli/init","unlisted":false},{"type":"link","label":"trainloop eval","href":"/reference/cli/eval","docId":"reference/cli/eval","unlisted":false},{"type":"link","label":"trainloop studio","href":"/reference/cli/studio","docId":"reference/cli/studio","unlisted":false},{"type":"link","label":"trainloop add","href":"/reference/cli/add","docId":"reference/cli/add","unlisted":false},{"type":"link","label":"trainloop benchmark","href":"/reference/cli/benchmark","docId":"reference/cli/benchmark","unlisted":false},{"type":"link","label":"Configuration","href":"/reference/cli/config","docId":"reference/cli/config","unlisted":false},{"type":"link","label":"Environment Variables","href":"/reference/cli/env-vars","docId":"reference/cli/env-vars","unlisted":false},{"type":"link","label":"trainloop upgrade","href":"/reference/cli/upgrade","docId":"reference/cli/upgrade","unlisted":false}],"href":"/reference/cli/"},{"type":"category","label":"SDKs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Python SDK API","href":"/reference/sdks/python-api","docId":"reference/sdks/python-api","unlisted":false},{"type":"link","label":"TypeScript SDK API","href":"/reference/sdks/typescript-api","docId":"reference/sdks/typescript-api","unlisted":false},{"type":"link","label":"Go SDK API","href":"/reference/sdks/go-api","docId":"reference/sdks/go-api","unlisted":false}],"href":"/reference/sdks/"},{"type":"category","label":"Data Formats","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Event Data Format","href":"/reference/data-formats/events","docId":"reference/data-formats/events","unlisted":false},{"type":"link","label":"Results Data Format","href":"/reference/data-formats/results","docId":"reference/data-formats/results","unlisted":false},{"type":"link","label":"Benchmark Data Formats","href":"/reference/data-formats/benchmark-data-formats","docId":"reference/data-formats/benchmark-data-formats","unlisted":false}],"href":"/reference/data-formats/"}],"href":"/reference/"},{"type":"category","label":"Tutorials","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quick Start Guide","href":"/tutorials/getting-started","docId":"tutorials/getting-started","unlisted":false},{"type":"link","label":"Writing Your First Evaluation","href":"/tutorials/first-evaluation","docId":"tutorials/first-evaluation","unlisted":false},{"type":"link","label":"Advanced Metrics with LLM Judge","href":"/tutorials/advanced-metrics","docId":"tutorials/advanced-metrics","unlisted":false},{"type":"link","label":"Benchmarking and Model Comparison","href":"/tutorials/benchmarking","docId":"tutorials/benchmarking","unlisted":false},{"type":"link","label":"Production Setup","href":"/tutorials/production-setup","docId":"tutorials/production-setup","unlisted":false}],"href":"/tutorials/"},{"type":"category","label":"Examples","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Python Examples","href":"/examples/python-examples","docId":"examples/python-examples","unlisted":false},{"type":"link","label":"TypeScript Examples","href":"/examples/typescript-examples","docId":"examples/typescript-examples","unlisted":false},{"type":"link","label":"Go Examples","href":"/examples/go-examples","docId":"examples/go-examples","unlisted":false}],"href":"/examples/"},{"type":"category","label":"Explanation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Architecture Overview","href":"/explanation/architecture","docId":"explanation/architecture","unlisted":false}],"href":"/explanation/"},{"type":"category","label":"Development","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Development Guide","href":"/development/","docId":"development/index","unlisted":false},{"type":"link","label":"SDK Testing Guide","href":"/development/sdk-testing","docId":"development/sdk-testing","unlisted":false},{"type":"link","label":"Release Process","href":"/development/release-process","docId":"development/release-process","unlisted":false},{"type":"link","label":"Architecture Guide","href":"/development/architecture","docId":"development/architecture","unlisted":false},{"type":"link","label":"Building from Source","href":"/development/building-from-source","docId":"development/building-from-source","unlisted":false},{"type":"link","label":"Code Style Guide","href":"/development/code-style","docId":"development/code-style","unlisted":false},{"type":"link","label":"Contributing Guide","href":"/development/contributing","docId":"development/contributing","unlisted":false},{"type":"link","label":"Local Development","href":"/development/local-development","docId":"development/local-development","unlisted":false},{"type":"link","label":"Pull Request Process","href":"/development/pull-request-process","docId":"development/pull-request-process","unlisted":false},{"type":"link","label":"Testing Guide","href":"/development/testing","docId":"development/testing","unlisted":false}],"href":"/category/development"}]},"docs":{"development/architecture":{"id":"development/architecture","title":"Architecture Guide","description":"This guide provides a comprehensive overview of the TrainLoop Evals architecture, including system design, component interactions, and data flow patterns.","sidebar":"docsSidebar"},"development/building-from-source":{"id":"development/building-from-source","title":"Building from Source","description":"This guide covers building TrainLoop Evals from source code, including all components and their dependencies.","sidebar":"docsSidebar"},"development/code-style":{"id":"development/code-style","title":"Code Style Guide","description":"This guide defines the coding standards and conventions used across the TrainLoop Evals project. Following these guidelines ensures consistency, readability, and maintainability.","sidebar":"docsSidebar"},"development/contributing":{"id":"development/contributing","title":"Contributing Guide","description":"Welcome to the TrainLoop Evals project! We\'re excited to have you contribute to our comprehensive LLM evaluation framework. This guide will help you get started with contributing to the project.","sidebar":"docsSidebar"},"development/index":{"id":"development/index","title":"Development Guide","description":"Use these pages to set up your local environment and understand how to contribute to TrainLoop.","sidebar":"docsSidebar"},"development/local-development":{"id":"development/local-development","title":"Local Development","description":"This guide walks you through setting up a complete development environment for TrainLoop Evals on your local machine.","sidebar":"docsSidebar"},"development/pull-request-process":{"id":"development/pull-request-process","title":"Pull Request Process","description":"This guide covers the complete pull request workflow for contributing to TrainLoop Evals, from preparation to merge.","sidebar":"docsSidebar"},"development/release-process":{"id":"development/release-process","title":"Release Process","description":"This document describes the TrainLoop Evals release process, including version bumping, changelog management, and publication workflows.","sidebar":"docsSidebar"},"development/sdk-testing":{"id":"development/sdk-testing","title":"SDK Testing Guide","description":"This document describes how to run tests for both Python and TypeScript SDKs in the TrainLoop Evals ecosystem.","sidebar":"docsSidebar"},"development/testing":{"id":"development/testing","title":"Testing Guide","description":"This guide covers the comprehensive testing framework used in TrainLoop Evals, including test categories, execution methods, and best practices.","sidebar":"docsSidebar"},"examples/go-examples":{"id":"examples/go-examples","title":"Go Examples","description":"Complete working example demonstrating TrainLoop LLM evaluation with Go.","sidebar":"docsSidebar"},"examples/index":{"id":"examples/index","title":"Examples","description":"Complete working examples demonstrating TrainLoop LLM evaluation across multiple programming languages.","sidebar":"docsSidebar"},"examples/python-examples":{"id":"examples/python-examples","title":"Python Examples","description":"Complete working example demonstrating TrainLoop LLM evaluation with Python.","sidebar":"docsSidebar"},"examples/typescript-examples":{"id":"examples/typescript-examples","title":"TypeScript Examples","description":"Complete working examples demonstrating TrainLoop LLM evaluation with TypeScript and JavaScript.","sidebar":"docsSidebar"},"explanation/architecture":{"id":"explanation/architecture","title":"Architecture Overview","description":"TrainLoop Evals is designed as a comprehensive evaluation framework that captures, processes, and analyzes LLM interactions. This document explains the system architecture and how all components work together.","sidebar":"docsSidebar"},"explanation/index":{"id":"explanation/index","title":"Understanding TrainLoop Evals","description":"This section provides in-depth explanations of TrainLoop Evals concepts, architecture, and design decisions. These guides help you understand the \\"why\\" behind TrainLoop Evals.","sidebar":"docsSidebar"},"getting-started/installation":{"id":"getting-started/installation","title":"Installation","description":"Get started with TrainLoop Evals by installing the CLI tool and SDKs. This guide covers all installation methods and platform-specific requirements.","sidebar":"docsSidebar"},"getting-started/quick-start":{"id":"getting-started/quick-start","title":"Quick Start Guide","description":"Get up and running with TrainLoop Evals in under 5 minutes. This guide walks you through setting up your first evaluation project, collecting LLM data, and running your first evaluations.","sidebar":"docsSidebar"},"guides/index":{"id":"guides/index","title":"How-to Guides","description":"This section will contain practical guides for common TrainLoop Evals tasks. In the meantime, you can find comprehensive guidance in these sections:","sidebar":"docsSidebar"},"intro":{"id":"intro","title":"Introduction to TrainLoop Evals","description":"Welcome to TrainLoop Evals, a comprehensive framework for automating the collection and evaluation of Large Language Model (LLM) outputs. TrainLoop Evals simplifies the process of testing and improving your AI applications with a focus on developer experience and reliability.","sidebar":"docsSidebar"},"reference/cli/add":{"id":"reference/cli/add","title":"trainloop add","description":"Add reusable metrics and suites from the TrainLoop registry to your project.","sidebar":"docsSidebar"},"reference/cli/benchmark":{"id":"reference/cli/benchmark","title":"trainloop benchmark","description":"Compare multiple LLM providers by re-running prompts and evaluating results with the same metrics.","sidebar":"docsSidebar"},"reference/cli/config":{"id":"reference/cli/config","title":"Configuration","description":"TrainLoop CLI configuration file format and options.","sidebar":"docsSidebar"},"reference/cli/env-vars":{"id":"reference/cli/env-vars","title":"Environment Variables","description":"Environment variables that control TrainLoop CLI behavior.","sidebar":"docsSidebar"},"reference/cli/eval":{"id":"reference/cli/eval","title":"trainloop eval","description":"Run evaluation suites to analyze your LLM interaction data and generate results.","sidebar":"docsSidebar"},"reference/cli/index":{"id":"reference/cli/index","title":"CLI Reference","description":"The TrainLoop CLI (trainloop) is the command-line interface for running evaluations, managing projects, and interacting with the TrainLoop Evals system.","sidebar":"docsSidebar"},"reference/cli/init":{"id":"reference/cli/init","title":"trainloop init","description":"Initialize a new TrainLoop project by creating the workspace structure and installing dependencies.","sidebar":"docsSidebar"},"reference/cli/studio":{"id":"reference/cli/studio","title":"trainloop studio","description":"Launch the Studio UI web interface for interactive visualization and analysis of evaluation results.","sidebar":"docsSidebar"},"reference/cli/upgrade":{"id":"reference/cli/upgrade","title":"trainloop upgrade","description":"Upgrade the TrainLoop project to the latest release and refresh generated files.","sidebar":"docsSidebar"},"reference/data-formats/benchmark-data-formats":{"id":"reference/data-formats/benchmark-data-formats","title":"Benchmark Data Formats","description":"The benchmark feature enables comparing evaluation results across multiple LLM providers. This document describes the data schema and JSONL output format for benchmark runs.","sidebar":"docsSidebar"},"reference/data-formats/events":{"id":"reference/data-formats/events","title":"Event Data Format","description":"TrainLoop SDKs collect LLM interaction data in a standardized JSONL format.","sidebar":"docsSidebar"},"reference/data-formats/index":{"id":"reference/data-formats/index","title":"Data Formats","description":"TrainLoop Evals uses standardized data formats for interoperability and vendor independence.","sidebar":"docsSidebar"},"reference/data-formats/results":{"id":"reference/data-formats/results","title":"Results Data Format","description":"TrainLoop CLI generates evaluation results in JSON format after processing events with metrics.","sidebar":"docsSidebar"},"reference/index":{"id":"reference/index","title":"Reference","description":"Complete API reference and technical documentation for TrainLoop Evals.","sidebar":"docsSidebar"},"reference/sdks/go-api":{"id":"reference/sdks/go-api","title":"Go SDK API","description":"The TrainLoop Evals Go SDK provides zero-touch instrumentation for Go applications.","sidebar":"docsSidebar"},"reference/sdks/index":{"id":"reference/sdks/index","title":"SDK Reference","description":"TrainLoop Evals provides SDKs for multiple languages to instrument your LLM applications with zero-touch instrumentation.","sidebar":"docsSidebar"},"reference/sdks/python-api":{"id":"reference/sdks/python-api","title":"Python SDK API","description":"The TrainLoop Evals Python SDK provides zero-touch instrumentation for Python applications.","sidebar":"docsSidebar"},"reference/sdks/typescript-api":{"id":"reference/sdks/typescript-api","title":"TypeScript SDK API","description":"The TrainLoop Evals TypeScript SDK provides zero-touch instrumentation for Node.js and TypeScript applications.","sidebar":"docsSidebar"},"tutorials/advanced-metrics":{"id":"tutorials/advanced-metrics","title":"Advanced Metrics with LLM Judge","description":"In this tutorial, you\'ll learn to use LLM Judge to create sophisticated evaluation metrics that go beyond simple rule-based checks. LLM Judge uses AI to evaluate AI, enabling complex quality assessments.","sidebar":"docsSidebar"},"tutorials/benchmarking":{"id":"tutorials/benchmarking","title":"Benchmarking and Model Comparison","description":"In this tutorial, you\'ll learn how to systematically compare different LLM providers to find the best model for your specific use case. We\'ll cover cost analysis, performance evaluation, and decision-making frameworks.","sidebar":"docsSidebar"},"tutorials/first-evaluation":{"id":"tutorials/first-evaluation","title":"Writing Your First Evaluation","description":"In this tutorial, you\'ll learn how to write effective evaluation metrics and organize them into comprehensive test suites. We\'ll build on the quick start guide to create more sophisticated evaluation criteria.","sidebar":"docsSidebar"},"tutorials/getting-started":{"id":"tutorials/getting-started","title":"Quick Start Guide","description":"Get up and running with TrainLoop Evals in under 5 minutes. This guide walks you through setting up your first evaluation project, collecting LLM data, and running your first evaluations.","sidebar":"docsSidebar"},"tutorials/index":{"id":"tutorials/index","title":"Tutorials","description":"Welcome to the TrainLoop Evals tutorials! These step-by-step guides will take you from complete beginner to advanced user.","sidebar":"docsSidebar"},"tutorials/production-setup":{"id":"tutorials/production-setup","title":"Production Setup","description":"This tutorial covers the essential considerations for running TrainLoop Evals in production environments.","sidebar":"docsSidebar"}}}}')}}]);