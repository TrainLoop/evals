"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9564],{7094:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"tutorials/first-evaluation","title":"Writing Your First Evaluation","description":"In this tutorial, you\'ll learn how to write effective evaluation metrics and organize them into comprehensive test suites. We\'ll build on the quick start guide to create more sophisticated evaluation criteria.","source":"@site/docs/tutorials/first-evaluation.md","sourceDirName":"tutorials","slug":"/tutorials/first-evaluation","permalink":"/tutorials/first-evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/tutorials/first-evaluation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docsSidebar","previous":{"title":"Quick Start Guide","permalink":"/tutorials/getting-started"},"next":{"title":"Advanced Metrics with LLM Judge","permalink":"/tutorials/advanced-metrics"}}');var i=s(4848),r=s(8453);const a={sidebar_position:3},l="Writing Your First Evaluation",o={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Understanding Evaluation Metrics",id:"understanding-evaluation-metrics",level:2},{value:"What Makes a Good Metric?",id:"what-makes-a-good-metric",level:3},{value:"Types of Metrics",id:"types-of-metrics",level:3},{value:"1. Rule-Based Metrics",id:"1-rule-based-metrics",level:4},{value:"2. Statistical Metrics",id:"2-statistical-metrics",level:4},{value:"3. Semantic Metrics",id:"3-semantic-metrics",level:4},{value:"Building Your Evaluation Suite",id:"building-your-evaluation-suite",level:2},{value:"Step 1: Define Your Quality Criteria",id:"step-1-define-your-quality-criteria",level:3},{value:"Step 2: Create Comprehensive Test Suites",id:"step-2-create-comprehensive-test-suites",level:3},{value:"Step 3: Create Focused Test Suites",id:"step-3-create-focused-test-suites",level:3},{value:"Advanced Metric Patterns",id:"advanced-metric-patterns",level:2},{value:"Using Context from the Request",id:"using-context-from-the-request",level:3},{value:"Metrics with Parameters",id:"metrics-with-parameters",level:3},{value:"Testing and Debugging Your Metrics",id:"testing-and-debugging-your-metrics",level:2},{value:"Test Individual Metrics",id:"test-individual-metrics",level:3},{value:"Run Tests Before Evaluation",id:"run-tests-before-evaluation",level:3},{value:"Running and Analyzing Results",id:"running-and-analyzing-results",level:2},{value:"Run Your Evaluation",id:"run-your-evaluation",level:3},{value:"Analyze Results in Studio UI",id:"analyze-results-in-studio-ui",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Start Simple, Add Complexity",id:"1-start-simple-add-complexity",level:3},{value:"2. Document Your Metrics",id:"2-document-your-metrics",level:3},{value:"3. Use Descriptive Names",id:"3-use-descriptive-names",level:3},{value:"4. Handle Edge Cases",id:"4-handle-edge-cases",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Metric Always Returns 0 or 1",id:"metric-always-returns-0-or-1",level:4},{value:"Metric Throws Errors",id:"metric-throws-errors",level:4},{value:"Inconsistent Results",id:"inconsistent-results",level:4},{value:"Debug Example",id:"debug-example",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"writing-your-first-evaluation",children:"Writing Your First Evaluation"})}),"\n",(0,i.jsx)(n.p,{children:"In this tutorial, you'll learn how to write effective evaluation metrics and organize them into comprehensive test suites. We'll build on the quick start guide to create more sophisticated evaluation criteria."}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"How to design effective evaluation metrics"}),"\n",(0,i.jsx)(n.li,{children:"Different types of metrics (rule-based, statistical, semantic)"}),"\n",(0,i.jsx)(n.li,{children:"How to organize metrics into logical test suites"}),"\n",(0,i.jsx)(n.li,{children:"Best practices for metric naming and documentation"}),"\n",(0,i.jsx)(n.li,{children:"How to debug and iterate on your metrics"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Completed the ",(0,i.jsx)(n.a,{href:"/tutorials/getting-started",children:"Quick Start Guide"})]}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of Python functions"}),"\n",(0,i.jsx)(n.li,{children:"An LLM application with collected data"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"understanding-evaluation-metrics",children:"Understanding Evaluation Metrics"}),"\n",(0,i.jsx)(n.h3,{id:"what-makes-a-good-metric",children:"What Makes a Good Metric?"}),"\n",(0,i.jsx)(n.p,{children:"A good evaluation metric should be:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Specific"})," - Tests one clear aspect of quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reliable"})," - Produces consistent results"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actionable"})," - Provides clear guidance for improvement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fast"})," - Runs quickly to enable rapid iteration"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"types-of-metrics",children:"Types of Metrics"}),"\n",(0,i.jsx)(n.h4,{id:"1-rule-based-metrics",children:"1. Rule-Based Metrics"}),"\n",(0,i.jsx)(n.p,{children:"Simple, deterministic checks based on patterns or rules:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def contains_required_elements(sample: Sample) -> int:\n    """Check if response contains required elements"""\n    response = sample.output.get("content", "")\n    required_elements = ["greeting", "name", "helpful"]\n    \n    for element in required_elements:\n        if element.lower() not in response.lower():\n            return 0\n    return 1\n'})}),"\n",(0,i.jsx)(n.h4,{id:"2-statistical-metrics",children:"2. Statistical Metrics"}),"\n",(0,i.jsx)(n.p,{children:"Metrics based on measurable properties:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def appropriate_length(sample: Sample) -> int:\n    """Check if response length is appropriate"""\n    response = sample.output.get("content", "")\n    word_count = len(response.split())\n    \n    # Adjust range based on your use case\n    return 1 if 10 <= word_count <= 100 else 0\n'})}),"\n",(0,i.jsx)(n.h4,{id:"3-semantic-metrics",children:"3. Semantic Metrics"}),"\n",(0,i.jsx)(n.p,{children:"Metrics that evaluate meaning and context:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def addresses_user_intent(sample: Sample) -> int:\n    """Check if response addresses the user\'s intent"""\n    user_message = ""\n    for msg in sample.input.get("messages", []):\n        if msg.get("role") == "user":\n            user_message = msg.get("content", "")\n            break\n    \n    response = sample.output.get("content", "")\n    \n    # Use simple keyword matching or more sophisticated NLP\n    if "question" in user_message.lower():\n        return 1 if "?" in response or "answer" in response.lower() else 0\n    \n    return 1  # Default pass for non-question inputs\n'})}),"\n",(0,i.jsx)(n.h2,{id:"building-your-evaluation-suite",children:"Building Your Evaluation Suite"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-define-your-quality-criteria",children:"Step 1: Define Your Quality Criteria"}),"\n",(0,i.jsx)(n.p,{children:'Before writing metrics, define what "good" looks like for your use case:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/metrics/content_quality.py\nfrom trainloop_cli.eval_core.types import Sample\n\ndef is_helpful(sample: Sample) -> int:\n    """Check if the response is helpful to the user"""\n    response = sample.output.get("content", "").lower()\n    \n    # Look for helpful indicators\n    helpful_indicators = [\n        "help", "assist", "support", "solution", "answer",\n        "explain", "guide", "suggest", "recommend"\n    ]\n    \n    return 1 if any(indicator in response for indicator in helpful_indicators) else 0\n\ndef is_accurate(sample: Sample) -> int:\n    """Check if the response contains accurate information"""\n    response = sample.output.get("content", "").lower()\n    \n    # Look for accuracy indicators (customize for your domain)\n    inaccurate_indicators = [\n        "i\'m not sure", "i don\'t know", "might be wrong",\n        "not certain", "unsure", "unclear"\n    ]\n    \n    return 0 if any(indicator in response for indicator in inaccurate_indicators) else 1\n\ndef follows_format(sample: Sample) -> int:\n    """Check if response follows expected format"""\n    response = sample.output.get("content", "")\n    \n    # Example: Check if response is properly structured\n    # Customize based on your format requirements\n    has_greeting = any(word in response.lower() for word in ["hello", "hi", "greetings"])\n    has_closing = any(word in response.lower() for word in ["thanks", "welcome", "help"])\n    \n    return 1 if has_greeting and has_closing else 0\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-create-comprehensive-test-suites",children:"Step 2: Create Comprehensive Test Suites"}),"\n",(0,i.jsx)(n.p,{children:"Organize your metrics into logical groupings:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/comprehensive_evaluation.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.content_quality import is_helpful, is_accurate, follows_format\nfrom ..metrics.greeting_quality import has_greeting_word, is_personalized, is_friendly_tone\n\n# Evaluate all LLM interactions\nresults = tag("").check(  # Empty tag evaluates all data\n    # Content quality metrics\n    is_helpful,\n    is_accurate,\n    follows_format,\n    \n    # Greeting-specific metrics (only applies to greeting calls)\n    has_greeting_word,\n    is_personalized,\n    is_friendly_tone\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-create-focused-test-suites",children:"Step 3: Create Focused Test Suites"}),"\n",(0,i.jsx)(n.p,{children:"Create specific suites for different types of interactions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/greeting_focused.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.greeting_quality import has_greeting_word, is_personalized, is_friendly_tone\n\n# Only evaluate greeting generation calls\nresults = tag("greeting-generation").check(\n    has_greeting_word,\n    is_personalized,\n    is_friendly_tone\n)\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/suites/customer_support.py\nfrom trainloop_cli.eval_core.helpers import tag\nfrom ..metrics.content_quality import is_helpful, is_accurate\nfrom ..metrics.support_specific import resolves_issue, shows_empathy\n\n# Only evaluate customer support interactions\nresults = tag("customer-support").check(\n    is_helpful,\n    is_accurate,\n    resolves_issue,\n    shows_empathy\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-metric-patterns",children:"Advanced Metric Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"using-context-from-the-request",children:"Using Context from the Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def matches_requested_tone(sample: Sample) -> int:\n    """Check if response matches the requested tone"""\n    system_message = ""\n    for msg in sample.input.get("messages", []):\n        if msg.get("role") == "system":\n            system_message = msg.get("content", "").lower()\n            break\n    \n    response = sample.output.get("content", "").lower()\n    \n    # Check if tone matches system instructions\n    if "formal" in system_message:\n        informal_words = ["hey", "sup", "yo", "gonna", "wanna"]\n        return 0 if any(word in response for word in informal_words) else 1\n    \n    if "casual" in system_message:\n        formal_words = ["furthermore", "consequently", "nevertheless"]\n        return 0 if any(word in response for word in formal_words) else 1\n    \n    return 1  # Default pass if no tone specified\n'})}),"\n",(0,i.jsx)(n.h3,{id:"metrics-with-parameters",children:"Metrics with Parameters"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def max_length_check(max_words: int):\n    """Create a metric that checks maximum length"""\n    def check_length(sample: Sample) -> int:\n        response = sample.output.get("content", "")\n        word_count = len(response.split())\n        return 1 if word_count <= max_words else 0\n    \n    return check_length\n\n# Usage in suite\nshort_response_check = max_length_check(50)\nmedium_response_check = max_length_check(200)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"testing-and-debugging-your-metrics",children:"Testing and Debugging Your Metrics"}),"\n",(0,i.jsx)(n.h3,{id:"test-individual-metrics",children:"Test Individual Metrics"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# trainloop/eval/metrics/test_greeting_quality.py\nfrom trainloop_cli.eval_core.types import Sample\nfrom .greeting_quality import has_greeting_word, is_personalized\n\ndef test_has_greeting_word():\n    # Test case 1: Response with greeting\n    sample1 = Sample(\n        input={},\n        output={"content": "Hello! How can I help you today?"}\n    )\n    assert has_greeting_word(sample1) == 1\n    \n    # Test case 2: Response without greeting\n    sample2 = Sample(\n        input={},\n        output={"content": "The weather is nice today."}\n    )\n    assert has_greeting_word(sample2) == 0\n    \n    print("\u2705 has_greeting_word tests passed")\n\nif __name__ == "__main__":\n    test_has_greeting_word()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"run-tests-before-evaluation",children:"Run Tests Before Evaluation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Test your metrics before running full evaluation\npython trainloop/eval/metrics/test_greeting_quality.py\n"})}),"\n",(0,i.jsx)(n.h2,{id:"running-and-analyzing-results",children:"Running and Analyzing Results"}),"\n",(0,i.jsx)(n.h3,{id:"run-your-evaluation",children:"Run Your Evaluation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Run all suites\ntrainloop eval\n\n# Run specific suite\ntrainloop eval --suite comprehensive_evaluation\n\n# Run with verbose output for debugging\ntrainloop eval --verbose\n"})}),"\n",(0,i.jsx)(n.h3,{id:"analyze-results-in-studio-ui",children:"Analyze Results in Studio UI"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"trainloop studio\n"})}),"\n",(0,i.jsx)(n.p,{children:"Look for:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Failing metrics"})," - Which criteria are not being met?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Patterns in failures"})," - Are failures clustered around specific inputs?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metric correlations"})," - Do certain metrics always pass/fail together?"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"1-start-simple-add-complexity",children:"1. Start Simple, Add Complexity"}),"\n",(0,i.jsx)(n.p,{children:"Begin with simple rule-based metrics and gradually add more sophisticated ones:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Start with this\ndef has_greeting(sample: Sample) -> int:\n    response = sample.output.get("content", "").lower()\n    return 1 if "hello" in response else 0\n\n# Evolve to this\ndef has_appropriate_greeting(sample: Sample) -> int:\n    response = sample.output.get("content", "").lower()\n    greetings = ["hello", "hi", "greetings", "good morning", "good afternoon", "good evening"]\n    return 1 if any(greeting in response for greeting in greetings) else 0\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-document-your-metrics",children:"2. Document Your Metrics"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def is_professional_tone(sample: Sample) -> int:\n    """\n    Check if the response maintains a professional tone.\n    \n    Criteria:\n    - Avoids slang and informal language\n    - Uses complete sentences\n    - Maintains respectful language\n    \n    Returns:\n        1 if professional tone is maintained, 0 otherwise\n    """\n    response = sample.output.get("content", "").lower()\n    \n    unprofessional_words = ["yo", "sup", "gonna", "wanna", "ain\'t"]\n    return 0 if any(word in response for word in unprofessional_words) else 1\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-use-descriptive-names",children:"3. Use Descriptive Names"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Good\ndef contains_required_safety_warning(sample: Sample) -> int:\n    pass\n\n# Bad\ndef check_safety(sample: Sample) -> int:\n    pass\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-handle-edge-cases",children:"4. Handle Edge Cases"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def has_valid_response(sample: Sample) -> int:\n    """Check if response is valid and non-empty"""\n    response = sample.output.get("content", "")\n    \n    # Handle edge cases\n    if not response:\n        return 0\n    \n    if response.strip() == "":\n        return 0\n    \n    if len(response) < 3:  # Too short to be meaningful\n        return 0\n    \n    return 1\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Congratulations! You now know how to write comprehensive evaluation metrics. Next, explore:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/tutorials/advanced-metrics",children:"Advanced Metrics with LLM Judge"})})," - Use AI to evaluate complex quality aspects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/tutorials/benchmarking",children:"Benchmarking and Model Comparison"})})," - Compare different LLM providers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/tutorials/production-setup",children:"Production Setup"})})," - Deploy evaluations in CI/CD pipelines"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsx)(n.h4,{id:"metric-always-returns-0-or-1",children:"Metric Always Returns 0 or 1"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Check your logic conditions"}),"\n",(0,i.jsx)(n.li,{children:"Add debug prints to see what data you're receiving"}),"\n",(0,i.jsx)(n.li,{children:"Test with known examples"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"metric-throws-errors",children:"Metric Throws Errors"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Add error handling for missing data"}),"\n",(0,i.jsx)(n.li,{children:"Check data types and structure"}),"\n",(0,i.jsx)(n.li,{children:"Use try/except blocks for robustness"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"inconsistent-results",children:"Inconsistent Results"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ensure your metric is deterministic"}),"\n",(0,i.jsx)(n.li,{children:"Check for race conditions in data access"}),"\n",(0,i.jsx)(n.li,{children:"Verify input data consistency"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"debug-example",children:"Debug Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def debug_metric(sample: Sample) -> int:\n    """Example of debugging a metric"""\n    try:\n        response = sample.output.get("content", "")\n        print(f"DEBUG: Response content: \'{response}\'")\n        \n        # Your metric logic here\n        result = 1 if "hello" in response.lower() else 0\n        print(f"DEBUG: Metric result: {result}")\n        \n        return result\n    except Exception as e:\n        print(f"DEBUG: Error in metric: {e}")\n        return 0\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Ready to build more sophisticated evaluations? Continue with ",(0,i.jsx)(n.a,{href:"/tutorials/advanced-metrics",children:"Advanced Metrics with LLM Judge"}),"!"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var t=s(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);