"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[976],{2053:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"intro","title":"Introduction to TrainLoop Evals","description":"Welcome to TrainLoop Evals, a comprehensive framework for automating the collection and evaluation of Large Language Model (LLM) outputs. TrainLoop Evals simplifies the process of testing and improving your AI applications with a focus on developer experience and reliability.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/evals/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/TrainLoop/evals/tree/main/docs/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Quick Start Guide","permalink":"/evals/getting-started/quick-start"},"next":{"title":"Guides","permalink":"/evals/category/guides"}}');var t=i(4848),r=i(8453);const o={sidebar_position:1},l="Introduction to TrainLoop Evals",a={},d=[{value:"What is TrainLoop Evals?",id:"what-is-trainloop-evals",level:2},{value:"Core Principles",id:"core-principles",level:2},{value:"\ud83c\udfaf Simplicity First",id:"-simplicity-first",level:3},{value:"\ud83d\udd04 Vendor Independence",id:"-vendor-independence",level:3},{value:"\ud83d\udc65 Meet Developers Where They Are",id:"-meet-developers-where-they-are",level:3},{value:"\ud83d\udd12 Type-safe, In-code Tests",id:"-type-safe-in-code-tests",level:3},{value:"\ud83e\udde9 Composable, Extensible System",id:"-composable-extensible-system",level:3},{value:"Key Features",id:"key-features",level:2},{value:"\ud83d\udcca Automatic Data Collection",id:"-automatic-data-collection",level:3},{value:"\ud83d\udd0d Powerful Evaluation Engine",id:"-powerful-evaluation-engine",level:3},{value:"\ud83d\udcc8 Rich Visualization",id:"-rich-visualization",level:3},{value:"\ud83d\ude80 Production Ready",id:"-production-ready",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Demo",id:"demo",level:2},{value:"Community and Support",id:"community-and-support",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"introduction-to-trainloop-evals",children:"Introduction to TrainLoop Evals"})}),"\n",(0,t.jsxs)(n.p,{children:["Welcome to ",(0,t.jsx)(n.strong,{children:"TrainLoop Evals"}),", a comprehensive framework for automating the collection and evaluation of Large Language Model (LLM) outputs. TrainLoop Evals simplifies the process of testing and improving your AI applications with a focus on developer experience and reliability."]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-trainloop-evals",children:"What is TrainLoop Evals?"}),"\n",(0,t.jsx)(n.p,{children:"TrainLoop Evals is an end-to-end evaluation framework that consists of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83e\udd16 CLI Tool"})," (",(0,t.jsx)(n.code,{children:"trainloop"})," command) - Python-based evaluation engine for managing evaluation workflows"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83c\udfa8 Studio UI"})," - Next.js web interface for visualizing and analyzing evaluation results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udcda Multi-language SDKs"})," - Zero-touch instrumentation libraries for Python, TypeScript, and Go"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udd27 Registry System"})," - Shareable metrics and evaluation suites for common use cases"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"core-principles",children:"Core Principles"}),"\n",(0,t.jsx)(n.p,{children:"TrainLoop Evals is built around five key principles:"}),"\n",(0,t.jsx)(n.h3,{id:"-simplicity-first",children:"\ud83c\udfaf Simplicity First"}),"\n",(0,t.jsx)(n.p,{children:"One environment variable, one function call, one folder of JSON files. No complex setup required."}),"\n",(0,t.jsx)(n.h3,{id:"-vendor-independence",children:"\ud83d\udd04 Vendor Independence"}),"\n",(0,t.jsx)(n.p,{children:"Everything is stored as newline-delimited JSON files. No databases, no vendor lock-in."}),"\n",(0,t.jsx)(n.h3,{id:"-meet-developers-where-they-are",children:"\ud83d\udc65 Meet Developers Where They Are"}),"\n",(0,t.jsx)(n.p,{children:"Accepts both simple declarative flows and existing bespoke evaluation loops."}),"\n",(0,t.jsx)(n.h3,{id:"-type-safe-in-code-tests",children:"\ud83d\udd12 Type-safe, In-code Tests"}),"\n",(0,t.jsx)(n.p,{children:"All evaluation code lives in your codebase with full type safety."}),"\n",(0,t.jsx)(n.h3,{id:"-composable-extensible-system",children:"\ud83e\udde9 Composable, Extensible System"}),"\n",(0,t.jsxs)(n.p,{children:["Helper generators follow proven patterns (similar to shadcn/ui) with ",(0,t.jsx)(n.code,{children:"trainloop add"})," command."]}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsx)(n.h3,{id:"-automatic-data-collection",children:"\ud83d\udcca Automatic Data Collection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-touch instrumentation"})," - Add one line to capture all LLM calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-language support"})," - Works with Python, TypeScript/JavaScript, and Go"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible tagging"})," - Tag specific calls for targeted evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Buffering control"})," - Configure immediate or batched data collection"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-powerful-evaluation-engine",children:"\ud83d\udd0d Powerful Evaluation Engine"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Custom metrics"})," - Write Python functions to evaluate any aspect of LLM output"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test suites"})," - Group related evaluations into logical collections"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM Judge"})," - Built-in AI-powered evaluation for subjective metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Benchmarking"})," - Compare multiple LLM providers on the same tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-rich-visualization",children:"\ud83d\udcc8 Rich Visualization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interactive Studio UI"})," - Explore evaluation results with charts and tables"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DuckDB integration"})," - Query evaluation data with SQL"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time updates"})," - See evaluation results as they happen"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Export capabilities"})," - Share and analyze results outside the platform"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-production-ready",children:"\ud83d\ude80 Production Ready"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalable architecture"})," - Handles large-scale evaluation workloads"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cloud storage support"})," - Works with S3, GCS, and Azure"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CI/CD integration"})," - Automate evaluations in your development pipeline"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Comprehensive testing"})," - Extensively tested across all components"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,t.jsx)(n.p,{children:"TrainLoop Evals is perfect for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udd27 Development Testing"})," - Continuously evaluate LLM outputs during development"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udcca A/B Testing"})," - Compare different prompts, models, or configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udd0d Quality Assurance"})," - Ensure LLM outputs meet quality standards before deployment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udcc8 Performance Monitoring"})," - Track LLM performance over time in production"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83c\udfc6 Model Comparison"})," - Benchmark different LLM providers and models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83c\udfaf Regression Testing"})," - Detect when changes negatively impact LLM performance"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udd27 Instrument"})," - Add TrainLoop SDK to your application with minimal code changes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udcdd Collect"})," - Automatically capture LLM requests and responses as JSONL files"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udcca Evaluate"})," - Define custom metrics and test suites to assess LLM performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udcc8 Analyze"})," - Use the Studio UI to visualize results and identify patterns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udd04 Iterate"})," - Refine your prompts and models based on evaluation insights"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Your App     \u2502    \u2502   TrainLoop     \u2502    \u2502   Studio UI     \u2502\n\u2502                 \u2502    \u2502   CLI           \u2502    \u2502                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502    SDK    \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u25b6\u2502   eval    \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u25b6\u2502   Viz     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502  LLM Calls      \u2502    \u2502  Metrics &      \u2502    \u2502  Charts &       \u2502\n\u2502  Auto-logged    \u2502    \u2502  Suites         \u2502    \u2502  Tables         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                       \u2502\n        \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   JSONL Files   \u2502    \u2502   Results       \u2502    \u2502   Interactive   \u2502\n\u2502   (events)      \u2502    \u2502   (verdicts)    \u2502    \u2502   Analysis      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsx)(n.p,{children:"Ready to start evaluating your LLM applications? Here's what you need to do:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/evals/getting-started/installation",children:"Install TrainLoop CLI"})})," - Get the command-line tool"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/evals/getting-started/quick-start",children:"Follow the Quick Start Guide"})})," - Set up your first evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./guides/",children:"Explore the Guides"})})," - Learn advanced features and best practices"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./reference/",children:"Check the Reference"})})," - Detailed API documentation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"demo",children:"Demo"}),"\n",(0,t.jsx)(n.p,{children:"Want to see TrainLoop Evals in action? Check out our demo:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://github.com/TrainLoop/chat-ui-demo",children:"Demo Repository"})})," - Complete example implementation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://evals.trainloop.ai",children:"Live Demo"})})," - Interactive demo deployment"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"community-and-support",children:"Community and Support"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://github.com/TrainLoop/trainloop-evals",children:"GitHub Repository"})})," - Source code and issues"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://github.com/TrainLoop/trainloop-evals/blob/main/CONTRIBUTING.md",children:"Contributing Guide"})})," - How to contribute"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://github.com/TrainLoop/trainloop-evals/blob/main/LICENSE",children:"License"})})," - MIT License"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Get started today and transform how you evaluate and improve your LLM applications!"})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);