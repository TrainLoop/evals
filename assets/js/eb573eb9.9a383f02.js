"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2393],{2705:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"reference/benchmark-schema","title":"Benchmark Data Schema","description":"The benchmark feature enables comparing evaluation results across multiple LLM providers. This document describes the data schema and JSONL output format for benchmark runs.","source":"@site/docs/reference/benchmark-schema.md","sourceDirName":"reference","slug":"/reference/benchmark-schema","permalink":"/reference/benchmark-schema","draft":false,"unlisted":false,"editUrl":"https://github.com/trainloop/evals/tree/main/docs/docs/reference/benchmark-schema.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docsSidebar","previous":{"title":"Results Data Format","permalink":"/reference/data-formats/results"},"next":{"title":"Understanding TrainLoop Evals","permalink":"/explanation/"}}');var a=r(4848),i=r(8453);const t={sidebar_position:5},o="Benchmark Data Schema",c={},l=[{value:"Design Principles",id:"design-principles",level:2},{value:"JSONL Output Format",id:"jsonl-output-format",level:2},{value:"1. Metadata Record (First Line)",id:"1-metadata-record-first-line",level:3},{value:"2. Result Records (Multiple Lines)",id:"2-result-records-multiple-lines",level:3},{value:"3. Summary Record (Last Line)",id:"3-summary-record-last-line",level:3},{value:"DuckDB View Schema",id:"duckdb-view-schema",level:2},{value:"Key Design Decisions",id:"key-design-decisions",level:2},{value:"UI Visualization Support",id:"ui-visualization-support",level:2},{value:"Future Extensions",id:"future-extensions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"benchmark-data-schema",children:"Benchmark Data Schema"})}),"\n",(0,a.jsx)(n.p,{children:"The benchmark feature enables comparing evaluation results across multiple LLM providers. This document describes the data schema and JSONL output format for benchmark runs."}),"\n",(0,a.jsx)(n.h2,{id:"design-principles",children:"Design Principles"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Compatibility"}),": Extends existing ",(0,a.jsx)(n.code,{children:"Sample"})," and ",(0,a.jsx)(n.code,{children:"Result"})," types rather than replacing them"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Flexibility"}),": Supports various comparison dimensions (providers, models, parameters)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"UI-Friendly"}),": Structured for efficient querying and visualization in DuckDB"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Extensibility"}),": Easy to add new metrics or provider configurations"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"jsonl-output-format",children:"JSONL Output Format"}),"\n",(0,a.jsxs)(n.p,{children:["Benchmark results are saved to ",(0,a.jsx)(n.code,{children:"data/benchmarks/{timestamp}/{suite_name}.jsonl"})," with three types of records:"]}),"\n",(0,a.jsx)(n.h3,{id:"1-metadata-record-first-line",children:"1. Metadata Record (First Line)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "type": "metadata",\n  "data": {\n    "benchmark_id": "bench_20240315_143022_abc123",\n    "timestamp": "2024-03-15T14:30:22.123Z",\n    "base_eval_run": "2024-03-15_14-25-00",\n    "suite_name": "qa_accuracy",\n    "description": "Comparing GPT-4 vs Claude vs Gemini on QA tasks",\n    "tags": ["production", "qa"],\n    "providers": [\n      {\n        "provider": "openai",\n        "model": "gpt-4",\n        "model_params": {"temperature": 0.0, "max_tokens": 1000}\n      },\n      {\n        "provider": "anthropic", \n        "model": "claude-3-opus-20240229",\n        "model_params": {"temperature": 0.0, "max_tokens": 1000}\n      }\n    ]\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-result-records-multiple-lines",children:"2. Result Records (Multiple Lines)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "type": "result",\n  "data": {\n    "provider_config": {\n      "provider": "openai",\n      "model": "gpt-4",\n      "model_params": {"temperature": 0.0}\n    },\n    "sample": {\n      "duration_ms": 1523,\n      "tag": "customer_support_001",\n      "input": [{"role": "user", "content": "How do I reset my password?"}],\n      "output": {"content": "To reset your password..."},\n      "model": "gpt-4",\n      "model_params": {"temperature": 0.0},\n      "start_time_ms": 1710515422000,\n      "end_time_ms": 1710515423523,\n      "url": "https://api.openai.com/v1/chat/completions",\n      "location": {"tag": "customer_support", "lineNumber": "42"}\n    },\n    "metrics": [\n      {\n        "metric": "response_quality",\n        "passed": 1,\n        "score": 0.92,\n        "reason": null\n      },\n      {\n        "metric": "hallucination_check", \n        "passed": 0,\n        "score": 0.3,\n        "reason": "Response contains unverified claim about password reset timeframe"\n      }\n    ],\n    "summary": {\n      "total_metrics": 2,\n      "passed_metrics": 1,\n      "avg_score": 0.61,\n      "pass_rate": 0.5\n    },\n    "timing": {\n      "provider_latency_ms": 1523,\n      "evaluation_time_ms": 342\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-summary-record-last-line",children:"3. Summary Record (Last Line)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "type": "summary",\n  "data": {\n    "benchmark_id": "bench_20240315_143022_abc123",\n    "timestamp": "2024-03-15T14:30:22.123Z",\n    "suite_name": "qa_accuracy",\n    "total_samples": 50,\n    "total_providers": 2,\n    "provider_summaries": {\n      "openai/gpt-4": {\n        "total_evaluations": 50,\n        "avg_pass_rate": 0.84,\n        "avg_latency_ms": 1456,\n        "total_cost": 2.34,\n        "metrics": {\n          "response_quality": {"pass_rate": 0.92, "avg_score": 0.89},\n          "hallucination_check": {"pass_rate": 0.76, "avg_score": 0.71}\n        }\n      },\n      "anthropic/claude-3-opus": {\n        "total_evaluations": 50,\n        "avg_pass_rate": 0.88,\n        "avg_latency_ms": 2103,\n        "total_cost": 3.12,\n        "metrics": {\n          "response_quality": {"pass_rate": 0.94, "avg_score": 0.91},\n          "hallucination_check": {"pass_rate": 0.82, "avg_score": 0.78}\n        }\n      }\n    },\n    "metric_comparisons": {\n      "response_quality": {\n        "best_provider": "anthropic/claude-3-opus",\n        "worst_provider": "openai/gpt-4",\n        "spread": 0.02\n      }\n    },\n    "overall": {\n      "best_provider": "anthropic/claude-3-opus",\n      "worst_provider": "openai/gpt-4",\n      "avg_duration_ms": 1779.5,\n      "total_duration_ms": 177950\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"duckdb-view-schema",children:"DuckDB View Schema"}),"\n",(0,a.jsxs)(n.p,{children:["The UI will create a ",(0,a.jsx)(n.code,{children:"benchmarks"})," view with the following structure:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE VIEW benchmarks AS\nSELECT\n  -- Extracted from filename/path\n  regexp_extract(filename, '/benchmarks/([^/]+)/', 1) AS ts,\n  regexp_extract(filename, '/benchmarks/[^/]+/([^/]+)\\.jsonl', 1) AS suite,\n  \n  -- Record type discrimination\n  type,\n  \n  -- Flattened fields for easier querying\n  data->>'benchmark_id' AS benchmark_id,\n  data->>'timestamp' AS benchmark_timestamp,\n  \n  -- For result records\n  data->'provider_config'->>'provider' AS provider,\n  data->'provider_config'->>'model' AS model,\n  data->'sample'->>'tag' AS sample_tag,\n  data->'summary'->>'avg_score' AS avg_score,\n  data->'summary'->>'pass_rate' AS pass_rate,\n  \n  -- Full JSON for detailed views\n  data\nFROM read_json_auto('${ROOT}/benchmarks/*/*.jsonl', filename=true);\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-design-decisions",children:"Key Design Decisions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Three-Record Structure"}),": Separates metadata, individual results, and summary for efficient processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Provider Namespacing"}),': Uses "provider/model" format for clear identification']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Metric Flexibility"}),": Supports both binary pass/fail and numeric scoring"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timing Separation"}),": Tracks both provider latency and evaluation overhead"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost Tracking"}),": Placeholder for future cost comparison features"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reference Linking"}),": Optional link to original evaluation run for traceability"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"ui-visualization-support",children:"UI Visualization Support"}),"\n",(0,a.jsx)(n.p,{children:"This schema enables the UI to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Show side-by-side provider comparisons"}),"\n",(0,a.jsx)(n.li,{children:"Generate charts for metric performance across providers"}),"\n",(0,a.jsx)(n.li,{children:"Calculate cost/performance tradeoffs"}),"\n",(0,a.jsx)(n.li,{children:"Filter by tags, suites, or specific providers"}),"\n",(0,a.jsx)(n.li,{children:"Drill down from summary to individual sample results"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"future-extensions",children:"Future Extensions"}),"\n",(0,a.jsx)(n.p,{children:"The schema is designed to support:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Multi-model comparisons within same provider"}),"\n",(0,a.jsx)(n.li,{children:"Parameter sensitivity analysis (temperature, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Cross-suite benchmark aggregation"}),"\n",(0,a.jsx)(n.li,{children:"Historical trend analysis"}),"\n",(0,a.jsx)(n.li,{children:"Custom metric plugins"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var s=r(6540);const a={},i=s.createContext(a);function t(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);