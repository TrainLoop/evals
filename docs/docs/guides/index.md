---
sidebar_position: 1
---

# TrainLoop Evals Guides

This section contains practical guides for using TrainLoop Evals effectively. Each guide focuses on accomplishing specific tasks and solving common use cases.

## Getting Started Guides

### SDK Integration
- **[Python SDK Guide](./python-sdk.md)** - Comprehensive Python integration
- **[TypeScript SDK Guide](./typescript-sdk.md)** - Complete TypeScript/JavaScript setup
- **[Go SDK Guide](./go-sdk.md)** - Go integration and best practices

### Basic Workflows
- **[Writing Metrics](./writing-metrics.md)** - Create custom evaluation functions
- **[Creating Test Suites](./creating-suites.md)** - Organize evaluations into logical groups
- **[Data Collection](./data-collection.md)** - Best practices for LLM data capture

## Advanced Features

### Evaluation Techniques
- **[LLM Judge Guide](./llm-judge.md)** - Using AI for subjective evaluations
- **[Benchmarking](./benchmarking.md)** - Compare multiple LLM providers
- **[Custom Metrics](./custom-metrics.md)** - Advanced metric development

### Studio UI
- **[Studio Overview](./studio-overview.md)** - Navigate the visualization interface
- **[Data Analysis](./data-analysis.md)** - Analyze evaluation results
- **[Exporting Data](./exporting-data.md)** - Export and share results

## Integration & Deployment

### CI/CD Integration
- **[GitHub Actions](./github-actions.md)** - Automate evaluations in CI
- **[GitLab CI](./gitlab-ci.md)** - GitLab integration guide
- **[Jenkins](./jenkins.md)** - Jenkins pipeline setup

### Production Deployment
- **[Cloud Storage](./cloud-storage.md)** - S3, GCS, and Azure integration
- **[Monitoring](./monitoring.md)** - Production monitoring setup
- **[Scaling](./scaling.md)** - Handle large-scale evaluations

## Use Case Examples

### Common Scenarios
- **[Prompt Engineering](./prompt-engineering.md)** - Evaluate and improve prompts
- **[Model Comparison](./model-comparison.md)** - Compare different LLM models
- **[Quality Assurance](./quality-assurance.md)** - QA workflows for LLM outputs
- **[Regression Testing](./regression-testing.md)** - Prevent quality degradation

### Domain-Specific
- **[Chatbot Evaluation](./chatbot-evaluation.md)** - Evaluate conversational AI
- **[Code Generation](./code-generation.md)** - Evaluate code-generating LLMs
- **[Content Creation](./content-creation.md)** - Evaluate creative content generation
- **[RAG Systems](./rag-systems.md)** - Evaluate retrieval-augmented generation

## Best Practices

### Development
- **[Project Structure](./project-structure.md)** - Organize your evaluation code
- **[Version Control](./version-control.md)** - Git best practices for evaluations
- **[Testing](./testing.md)** - Test your evaluation code

### Performance
- **[Optimization](./optimization.md)** - Optimize evaluation performance
- **[Caching](./caching.md)** - Cache evaluation results
- **[Parallel Processing](./parallel-processing.md)** - Run evaluations in parallel

## Migration & Compatibility

### Migration Guides
- **[From Other Tools](./migration.md)** - Migrate from other evaluation frameworks
- **[Upgrading](./upgrading.md)** - Upgrade between TrainLoop versions
- **[Legacy Support](./legacy-support.md)** - Work with legacy systems

## Troubleshooting

### Common Issues
- **[Debugging](./debugging.md)** - Debug evaluation issues
- **[Performance Issues](./performance-issues.md)** - Solve performance problems
- **[Error Handling](./error-handling.md)** - Handle and recover from errors

## Coming Soon

We're actively working on comprehensive guides for each of these topics. Guides will be added based on community feedback and common use cases.

## Contributing

Want to contribute a guide? We welcome community contributions! Please see our [Contributing Guide](https://github.com/trainloop/evals/blob/main/CONTRIBUTING.md) for details.

## Questions?

If you need help with a specific use case:

- Check the [Reference](../reference/) for API details
- Review the [Explanation](../explanation/) for conceptual understanding
- [Open an issue](https://github.com/trainloop/evals/issues) for questions